<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="sean lee`s blog" />










<meta name="description" content="NLP / DL / Python / C++ | 爱我所爱">
<meta property="og:type" content="website">
<meta property="og:title" content="明天探索者">
<meta property="og:url" content="http://seanlee97.github.io/index.html">
<meta property="og:site_name" content="明天探索者">
<meta property="og:description" content="NLP / DL / Python / C++ | 爱我所爱">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="明天探索者">
<meta name="twitter:description" content="NLP / DL / Python / C++ | 爱我所爱">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://seanlee97.github.io/"/>





  <title>明天探索者</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">明天探索者</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/11/18/基于kd树的knn实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/18/基于kd树的knn实现/" itemprop="url">基于kd树的knn实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-18T10:39:49+08:00">
                2018-11-18
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/18/基于kd树的knn实现/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/11/18/基于kd树的knn实现/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>K-NN(K-Nearest Neighbor) 是一种经典的分类算法，相信你已经对算法有所了解了。KNN有一种经典实现 —— KD 树实现，但是却不是太好理解，因为 KD 树涉及到了高维空间描述，大家都知道三维空间的生物理解高维空间的事物都是很困难的。</p>
<p>本文主要讲解 KD 树版 KNN 实现，代码主要使用 Python.</p>
<h2 id="knn-的简单理解">KNN 的简单理解</h2>
<p>形象地说 KNN 利用的是 <code>近朱者赤近墨者黑</code> 的原理，比如要判断一个人的性格，KNN 不对这个人本身直接分析判断，而是分析这个人所在的群体进行判断（因为往往群体的信息量更大，更好地采集数据）。</p>
<p><img src="/images/posts/knn_kd/knn.png"></p>
<h2 id="kd-树的简单理解">KD 树的简单理解</h2>
<blockquote>
<p>在计算机科学里，k-d树（ k-dimension tree）是在k维欧几里德空间组织点的数据结构。 — 维基百科</p>
</blockquote>
<p>k-d 树是每个节点都为 k 维点的<strong>二叉树</strong>，简单理解就是原始的二叉树存储的是一个值，但现在存储的是一个在 k 维空间上的点，作为三维空间的人类为了保护脑细胞<strong>本文均以二维空间举例</strong>，也就是说本文中二叉树结点存储的是二维空间上的点，即平面直角坐标系上的点 (x, y)</p>
<p>k-d 树中所有<strong>非叶子节点</strong>可以视作用一个超平面把空间分割成两个半空间 (也就是左右子树分别存储着两个半空间)：节点左边的子树代表在超平面左边的点，节点右边的子树代表在超平面右边的点。</p>
<p>选择超平面的方法如下： 每个节点都与 k 维中<strong>垂直于</strong>超平面的那一维有关。 因此，如果选择按照 x 轴划分，所有 x 值小于指定值 (为了高效的利用二叉树的空间，这个值一般是<strong>中位数</strong>) 的节点都会出现在左子树，所有 x 值大于指定值的节点都会出现在右子树。 这样，超平面可以用该x值来确定，其法线为x轴的单位向量。</p>
<p><strong>为了使得构造的 kd 树尽量平衡，一般会依次切换坐标轴</strong>，也就是说：如果当前结点是按 x 轴切分，那么它的子结点是按 y 轴切分的 (再次声明，本文是建立在二维空间基础上的)</p>
<h2 id="算法流程">算法流程</h2>
<p><strong>输入:</strong> 二维空间的数据集 T = { <span class="math inline">\((x_{1}, y_{1})\)</span>, <span class="math inline">\((x_{2}, y_{2})\)</span>, …, <span class="math inline">\((x_{n}, y_{y})\)</span> }</p>
<h3 id="kd-树构造过程">KD 树构造过程:</h3>
<ol type="1">
<li>构造根结点：先选择 x 轴作为最初的坐标轴，对当前空间上的点按照 x 坐标值进行排序，选择中位数所在的 y 轴作为切分超平面，超平面左边的点由左子树存储，右边的点由右子树存储</li>
</ol>
<p><img src="/images/posts/knn_kd/01.gif"></p>
<ol start="2" type="1">
<li>递归：对左右子树进行和 1. 一样的递归操作，不过坐标轴要切换 (父节点选择的坐标轴是 x 轴的话，那么子结点是 y 轴，反之是 x 轴)</li>
<li>结束：直到两个子区域没有实例了即可停止</li>
</ol>
<p>整个过程如下图所示</p>
<p><img src="/images/posts/knn_kd/02.png"></p>
<h2 id="实现步骤">实现步骤</h2>
<h3 id="构建-kd-树">构建 kd 树</h3>
<p>既然 kd 树是一棵二叉树，那么首先想到的是递归构造了，之前已经说了根结点取当前子空间在所选坐标轴上的中位数，所以我们要对当前子空间上的点的所选轴坐标的值进行排序，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_kdtree</span><span class="params">(points, depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在这里我们用depth来保存当前结点的深度，</span></span><br><span class="line"><span class="string">    深度主要用来进行坐标轴的切换</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    n = len(points)</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 如果当前子空间已经没有点了则构建过程结束</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算当前选择的坐标轴</span></span><br><span class="line">    axis = depth % <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对当前子空间的点根据当前选择轴的值进行排序</span></span><br><span class="line">    sorted_points = sorted(points, key=<span class="keyword">lambda</span> point: point[axis])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 中位数取排序后坐标点的中间位置的数</span></span><br><span class="line">    median = n // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="comment"># 当前根结点</span></span><br><span class="line">        <span class="string">"point"</span>: sorted_points[median],</span><br><span class="line">        <span class="comment"># 将超平面左边的点交由左子结点递归操作</span></span><br><span class="line">        <span class="string">"left"</span>: build_kdtree(sorted[:median], depth+<span class="number">1</span>),</span><br><span class="line">        <span class="comment"># 同理，将超平面右边的点交由右子结点递归操作</span></span><br><span class="line">        <span class="string">"right"</span>: build_kdtree(sorted_points[median+<span class="number">1</span>:], depth+<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="找邻近点1距离计算">找邻近点1：距离计算</h3>
<p>因为要找邻近的点，所以需要距离来很衡量，平面直角坐标系中两点 <span class="math inline">\(p_{1}(x_{1}, y_{1}), p2(x_{2}, y_{2})\)</span> 距离的计算公式为:</p>
<p><span class="math display">\[dist = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}\]</span></p>
<p>Python 实现也很简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(point1, point2)</span>:</span></span><br><span class="line">    x1, y1 = point1</span><br><span class="line">    x2, y2 = point2</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># math 的 sqrt(*) 用来计算平方根</span></span><br><span class="line">    dist = math.sqrt( (x2-x1)**<span class="number">2</span> + (y2-y1)**<span class="number">2</span> )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure>
<h3 id="找邻近点2找到两结点离当前点更近的结点">找邻近点2：找到两结点离当前点更近的结点</h3>
<p>为什么要做这一步呢？因为遍历二叉树的时候我们需要将当前点分别和根结点、左子结点、右子结点的距离进行比较。该函数实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closer_distance</span><span class="params">(point, p1, p2)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> p1 <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> p2</span><br><span class="line">    <span class="keyword">if</span> p2 <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> p1</span><br><span class="line">        </span><br><span class="line">    d1 = distance(point, p1)</span><br><span class="line">    d2 = distance(point, p2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> p1 <span class="keyword">if</span> d1 &lt; d2 <span class="keyword">else</span> p2</span><br></pre></td></tr></table></figure>
<h3 id="找邻近点3从根结点出发遍历-kd-树找到与给定点最近的结点">找邻近点3：从根结点出发，遍历 kd 树，找到与给定点最近的结点</h3>
<p>这一步是找邻近点最后一步，不做过多解释，代码看懂了也就理解了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kdtree_closest_point</span><span class="params">(root, point, depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定当前轴</span></span><br><span class="line">    axis = depth % <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    next_branch = <span class="keyword">None</span></span><br><span class="line">    opposite_branch = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> point[axis] &lt; root[<span class="string">'point'</span>][axis]:</span><br><span class="line">        next_branch = root[<span class="string">'left'</span>]</span><br><span class="line">        opposite_branch = root[<span class="string">'right'</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_branch = root[<span class="string">'right'</span>]</span><br><span class="line">        opposite_branch = root[<span class="string">'left'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以下主要是比较当前点到根结点和两个子结点之间的距离</span></span><br><span class="line"></span><br><span class="line">    best = closer_distance(</span><br><span class="line">    	point,</span><br><span class="line">        kdtree_closest_point(</span><br><span class="line">            next_branch,</span><br><span class="line">            point,</span><br><span class="line">            depth + <span class="number">1</span>),</span><br><span class="line">        root[<span class="string">'point'</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当前结点和根结点在所选轴上的距离</span></span><br><span class="line">    <span class="comment"># abs(*) ： 取绝对值</span></span><br><span class="line">    cnt_dist = abs(point[axis] - root[<span class="string">'point'</span>][axis])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> distance(point, best) &gt; cnt_dist:</span><br><span class="line"></span><br><span class="line">        best = closer_distance(</span><br><span class="line">            point,</span><br><span class="line">            kdtree_closest_point(</span><br><span class="line">                opposite_branch,</span><br><span class="line">                point,</span><br><span class="line">                depth + <span class="number">1</span>),</span><br><span class="line">            best</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best</span><br></pre></td></tr></table></figure>
<h3 id="完整代码">完整代码</h3>
<p>knn 预测时往往不是拿最邻近点的标签作为最后的预测（避免出现<code>鹤立鸡群</code>的情况），而是取 topk 个距离最近的点通过投票的机制来选择票数最高的标签作为最后的输出</p>
<p><img src="/images/posts/knn_kd/03.png"></p>
<p>完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, topk=<span class="number">3</span>)</span>:</span></span><br><span class="line">        self.data = <span class="keyword">None</span></span><br><span class="line">        self.store = &#123;&#125;</span><br><span class="line">        self.topk = topk</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_kdtree</span><span class="params">(self, points, depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">        n = len(points)</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 如果当前子空间已经没有点了则构建过程结束</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算当前选择的坐标轴</span></span><br><span class="line">        axis = depth % <span class="number">2</span></span><br><span class="line">        <span class="comment"># 对当前子空间的点根据当前选择轴的值进行排序</span></span><br><span class="line">        sorted_points = sorted(points, key=<span class="keyword">lambda</span> point: point[axis])</span><br><span class="line">        <span class="comment"># 中位数取排序后坐标点的中间位置的数</span></span><br><span class="line">        median = n // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="comment"># 当前根结点</span></span><br><span class="line">            <span class="string">'point'</span>: sorted_points[median],</span><br><span class="line">            <span class="comment"># 将超平面左边的点交由左子结点递归操作</span></span><br><span class="line">            <span class="string">'left'</span>: self.build_kdtree(sorted_points[:median], depth+<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># 同理，将超平面右边的点交由右子结点递归操作</span></span><br><span class="line">            <span class="string">'right'</span>: self.build_kdtree(sorted_points[median+<span class="number">1</span>:], depth+<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(self, p1, p2)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> p1 <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> p2 <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        x1, y1 = p1</span><br><span class="line">        x2, y2 = p2</span><br><span class="line"></span><br><span class="line">        x_ = x2 - x1</span><br><span class="line">        y_ = y2 - y1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> math.sqrt(x_**<span class="number">2</span> + y_**<span class="number">2</span>)    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closer_distance</span><span class="params">(self, point, p1, p2)</span>:</span></span><br><span class="line"></span><br><span class="line">        d1 = self.distance(point, p1)</span><br><span class="line">        d2 = self.distance(point, p2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> p1 <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> (p2, d2)</span><br><span class="line">        <span class="keyword">if</span> p2 <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> (p1, d1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (p1, d1) <span class="keyword">if</span> d1 &lt; d2 <span class="keyword">else</span> (p2, d2)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kdtree_closest_point</span><span class="params">(self, root, point, depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">            </span><br><span class="line">        axis = depth % <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        next_branch = <span class="keyword">None</span></span><br><span class="line">        opposite_branch = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 以下主要是比较当前点到根结点和两个子结点之间的距离</span></span><br><span class="line">        <span class="keyword">if</span> point[axis] &lt; root[<span class="string">'point'</span>][axis]:</span><br><span class="line">            next_branch = root[<span class="string">'left'</span>]</span><br><span class="line">            opposite_branch = root[<span class="string">'right'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_branch = root[<span class="string">'right'</span>]</span><br><span class="line">            opposite_branch = root[<span class="string">'left'</span>]</span><br><span class="line">        </span><br><span class="line">        best, closer_dist = self.closer_distance(</span><br><span class="line">            point, </span><br><span class="line">            self.kdtree_closest_point(</span><br><span class="line">                next_branch, </span><br><span class="line">                point, </span><br><span class="line">                depth + <span class="number">1</span>),</span><br><span class="line">            root[<span class="string">'point'</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.distance(point, best) &gt; abs(point[axis] - root[<span class="string">'point'</span>][axis]):</span><br><span class="line"></span><br><span class="line">            best, closer_dist = self.closer_distance(</span><br><span class="line">                point, </span><br><span class="line">                self.kdtree_closest_point(</span><br><span class="line">                    opposite_branch,</span><br><span class="line">                    point,</span><br><span class="line">                    depth + <span class="number">1</span>),</span><br><span class="line">                best</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 储存距离，留作投票用</span></span><br><span class="line">        <span class="keyword">if</span> best <span class="keyword">in</span> self.store <span class="keyword">and</span> self.store[best] &gt; closer_dist:</span><br><span class="line">            self.store[best] =  closer_dist</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.store[best] = closer_dist</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.data = dict(zip(X, y))</span><br><span class="line">        self.kdtree = self.build_kdtree(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, point)</span>:</span></span><br><span class="line">        <span class="comment"># best 是最邻近的点</span></span><br><span class="line">        best = self.kdtree_closest_point(self.kdtree, point)</span><br><span class="line"></span><br><span class="line">        sorted_stores = sorted(self.store.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[:self.topk]</span><br><span class="line">        counter = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> candidates, score <span class="keyword">in</span> sorted_stores:</span><br><span class="line">            counter[self.data[candidates]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按照投票数降序排列</span></span><br><span class="line">        sorted_counter = sorted(counter.items(), key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line">        counter = list(counter.items())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(counter) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> counter[<span class="number">0</span>][<span class="number">1</span>] != counter[<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">                best = counter[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.data[best]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练数据</span></span><br><span class="line">    points = [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1.2</span>), (<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0.2</span>), (<span class="number">3</span>, <span class="number">0.5</span>), (<span class="number">3.3</span>, <span class="number">0.9</span>)]</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'C'</span>]</span><br><span class="line"></span><br><span class="line">    knn = KNN(topk=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    knn.fit(points, labels)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    label = knn.predict((<span class="number">0.9</span>,<span class="number">0.9</span>))</span><br><span class="line">    print(label)</span><br></pre></td></tr></table></figure>
<p>代码也可查看 <a href="https://github.com/SeanLee97/datastruct_and_algorithms/tree/master/ml/knn" target="_blank" rel="noopener">github / knn</a></p>
<h2 id="refrence">Refrence</h2>
<ul>
<li>《统计学习方法》</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/implementing-kd-tree-for-fast-range-search-nearest-neighbor" target="_blank" rel="noopener">implementing-kd-tree-for-fast-range-search-nearest-neighbor</a></li>
<li><a href="https://zh.wikipedia.org/wiki/K-d%E6%A0%91" target="_blank" rel="noopener">k-d 树</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/11/15/深度学习那些事/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/15/深度学习那些事/" itemprop="url">深度学习那些事</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-15T13:36:28+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/深度学习那些事/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/11/15/深度学习那些事/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>现今使用深度学习的方法解决 NLP 领域的问题变得越来越流行，近年来，学界上出现了越来越多有关深度学习的论文;业界上各种深度学习框架、深度学习应用也被推出。</p>
<p>本文主要分享本人对于深度学习的理解及一些经验。</p>
<h2 id="大纲">大纲</h2>
<ul>
<li>什么是深度学习？</li>
<li>热门的深度学习框架及选择建议</li>
<li>常用深度学习模块原理及应用
<ul>
<li>CNN</li>
<li>RNN及其变体</li>
<li>Attention机制的原理以及实现</li>
<li>Transformer</li>
</ul></li>
<li>常用的网络训练技巧
<ul>
<li>正则化的意义及使用方式</li>
<li>规范化的作用及常用的规范化方式</li>
<li>常见的激活函数及选择</li>
<li>常见的优化器及选择</li>
<li>mask 的作用</li>
<li>模型效果提升偏方
<ul>
<li>迁移学习</li>
<li>鉴别学习</li>
<li>模型融合</li>
<li>模型蒸馏</li>
</ul></li>
</ul></li>
<li>深度学习与NLP
<ul>
<li>word2vec vs fasttext</li>
<li>如何给网络添加特征</li>
<li>ELMo、GPT、BERT</li>
</ul></li>
<li>TensorFlow 实战：以DPCNN为例讲述如何实现一篇论文</li>
<li>写在最后</li>
<li>Refrence</li>
</ul>
<h2 id="什么是深度学习">什么是深度学习？</h2>
<p>深度学习 <span class="math inline">\(\neq\)</span> 深度学习框架，掌握深度学习首先要掌握相关的理论知识，掌握常用的网络模型，锻炼自己的工程能力。做到看得懂论文，还能自己实现论文。</p>
<p>现在流行的深度学习方法主要是基于反向传播(Back-Propagation, BP)思想的，主要是<strong>深度神经网络</strong> (DNN，Deep Neural Network)，深度学习一般在大数据量下才能体现出其优点，而且深度学习对计算资源要求也比较高，一般需要用到 GPU (Graphics processing unit) 加速运算。</p>
<p><img src="/images/posts/deeplearning/01.png"></p>
<h3 id="神经网络">神经网络</h3>
<h4 id="神经网络表示">神经网络表示</h4>
<ul>
<li>输入层：输入特征被称作神经网络的输入层 (Input Layer)</li>
<li>隐藏层：一般在输入层和输出层之间的网络层都可称为隐藏层，“隐藏”的含义是中间节点的真正数值是无法看到的。</li>
<li>输出层：将网络空间最终映射到<code>任务空间</code></li>
</ul>
<p><img src="/images/posts/deeplearning/dp01.png"></p>
<p>如图是一个双层神经网络(一般计算网络的层数时，通常不考虑输入层，因此图中隐藏层是第一层，输出层是第二层)，也称作<strong>单隐层神经网络</strong>，隐藏层中每个结点称为<strong>神经元</strong>，神经元是含有非线性激活函数的感知单元。</p>
<p>其中 <span class="math inline">\(a^{[0]}, a^{[1]}, .., a^{[layer\_size]}\)</span> 等为每一层的输出值，如 <span class="math inline">\(a^{[1]} = [a_{0}^{[1]}, a_{1}^{[1]}, ..., a_{hidden\_size}^{[1]}]\)</span> 为第一层的输出值。</p>
<h4 id="神经网络的计算">神经网络的计算</h4>
<p>神经网络中的计算由前向传播与反向传播构成，一般由计算图表示。</p>
<h5 id="前向传播">前向传播</h5>
<p>前向传播是计算神经网络输出的过程，对于单个神经元有：</p>
<p><img src="/images/posts/deeplearning/dp02.png"></p>
<p>其中 <span class="math inline">\(W\)</span> 是一个向量，<span class="math inline">\(W^{T}\)</span> 是向量的转置， <span class="math inline">\(b\)</span> 是一个标量</p>
<p>对于隐藏层的第一个结点有：</p>
<p><span class="math display">\[z_{1}^{[1]} = (W_{1}^{[1]})^{T}X + b_{1}^{[1]} \\\\
a_{1}^{[1]} = \sigma(z_{1}^{[1]}) \\\\
where\  X = \begin{bmatrix}
x_{1} \\\\ 
x_{2} \\\\ 
x_{3}
\end{bmatrix}, W_{1}^{[1]} \in \mathbb{R}^{3\times 1}, b_{1}^{[1]}\ is\ a\ scalar\]</span></p>
<p>依次类推对于第一个隐藏层整体有：</p>
<p><span class="math display">\[z^{[1]} = (W^{[1]})^{T} a^{[0]} + b^{[1]} \\\\
a^{[1]} = \sigma(z^{[1]}) \\\\
where\ (W^{[1]})^{T} = \begin{bmatrix}
(W_{1}^{[1]})^{T} \\\\ 
(W_{2}^{[1]})^{T} \\\\ 
(W_{3}^{[1]})^{T} \\\\
(W_{4}^{[1]})^{T}
\end{bmatrix} \in \mathbb{R}^{4\times 3}, b^{[1]} = \begin{bmatrix}
b_{1}^{[1]} \\\\ 
b_{2}^{[1]} \\\\ 
b_{3}^{[1]} \\\\
b_{4}^{[1]}
\end{bmatrix} \in \mathbb{R}^{4\times 1}\]</span></p>
<p>同理对于输出层有</p>
<p><span class="math display">\[z^{[2]} = (W^{[2]})^{T} a^{[1]} + b^{[2]} \\\\
\hat{y} = a^{[2]} = \sigma(z^{[2]}) \\\\
where\ (W^{[2]})^{T} \in \mathbb{R}^{1\times 4}, b^{[2]}\in \mathbb{R}^{1\times 1}\]</span></p>
<h5 id="反向传播">反向传播</h5>
<p>所谓的反向传播（Back Propagation）即是当我们需要计算最终值相对于某个特征变量的导数时，我们需要利用计算图中上一步的结点定义。</p>
<p>如图</p>
<p><img src="/images/posts/deeplearning/dp03.png"></p>
<p>其中 <span class="math inline">\(L(a, y)\)</span> 为损失函数，损失函数计算了预测值 <span class="math inline">\(a\)</span> 与真实值 <span class="math inline">\(y\)</span> 的差距。这里假设 <span class="math inline">\(L(a, y)\)</span> 损失函数为</p>
<p><span class="math display">\[L(a, y) = -(y log a) - (1-y)log(1-a)\]</span></p>
<p>反向传播过程如下：</p>
<p>首先反向求出 <span class="math inline">\(L\)</span> 对于 <span class="math inline">\(a\)</span> 的导数：</p>
<p><span class="math display">\[d(a) = \frac{d L(a, y)}{da} = - \frac{y}{a} + \frac{1-y}{1-a}\]</span></p>
<p>然后继续反向求出 <span class="math inline">\(L\)</span> 对 <span class="math inline">\(z\)</span> 的导数，根据链式求导法则有</p>
<p><span class="math display">\[dz = \frac{dL}{dz} = \frac{dL}{da}\frac{da}{dz} = a-y\]</span></p>
<p>依次类推求出最终损失函数相对于<strong>原始参数</strong>的导数之后，使用梯度下降方式优化更新参数</p>
<p><span class="math display">\[w_{1} := w_{1} - \alpha dw_{1} \\\\
w_{2} := w_{2} - \alpha dw_{2} \\\\
b := b - \alpha db\]</span></p>
<p>其中 <span class="math inline">\(\alpha\)</span> 为优化器的学习速率</p>
<h4 id="神经网络学习建议">神经网络学习建议</h4>
<p>建议自己动手实现一个简易的神经网络框架 (可以基于 python 的 numpy 库) 实现前向传播和反向传播过程，这样有助于对神经网络的理解。</p>
<h2 id="热门的深度学习框架及选择建议">热门的深度学习框架及选择建议</h2>
<p>深度学习框架主要有两种类型：静态图和动态图，静态图的代表框架是 Google 开源的 TensorFlow (简称 TF) , 动态图的代表框架是 Facebook 开源的 PyTorch (简称 PT)，上述两种框架也是如今最热门的两种框架生态。</p>
<p>以 TensorFlow 为代表的框架生态主要有：</p>
<ul>
<li>TensorFlow</li>
<li>TensorFlow Edge: TF 的动态图版本</li>
<li>Keras: 基于 TF 的高级模块框架，已被高度继承在新版的 TensorFlow 中</li>
</ul>
<p>以 PyTorch 为代表的框架生态主要有：</p>
<ul>
<li>PyTorch</li>
<li>fast.ai：基于 PT 的高级模块框架</li>
<li>cafee：常用于计算机视觉领域，已集成到新版 PyTorch 中</li>
</ul>
<h3 id="静态图和动态图的区别">静态图和动态图的区别</h3>
<h4 id="静态图">静态图</h4>
<p>静态图把模型和数据分离开来，必须先用占位结点 (placeholder) 构建好计算图，然后才能将数据“喂入” (feed)，在数据没有流入前是很难对网络进行调试的，这也是静态图的缺点。</p>
<h4 id="动态图">动态图</h4>
<p>可以随时捕获网络当前的状态，便于调试</p>
<h3 id="两种框架的对比">两种框架的对比</h3>
<table>
<thead>
<tr class="header">
<th>—</th>
<th>TensorFlow</th>
<th>PyTorch</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>计算图类型</td>
<td>静态图</td>
<td>动态图</td>
</tr>
<tr class="even">
<td>学习成本</td>
<td>不易入门</td>
<td>易入门</td>
</tr>
<tr class="odd">
<td>执行效率</td>
<td>适合分布式</td>
<td>单机效率强大，缺点是CPU，GPU切换复杂</td>
</tr>
<tr class="even">
<td>调试</td>
<td>不易调试</td>
<td>易调试</td>
</tr>
<tr class="odd">
<td>部署</td>
<td>易部署，友好支持多终端，支持分布式</td>
<td>还在完善中</td>
</tr>
<tr class="even">
<td>生态</td>
<td>背靠 Google AI / Brain，生态强大</td>
<td>生态还在完善中</td>
</tr>
</tbody>
</table>
<h3 id="选择建议">选择建议</h3>
<p>个人一开始学习的是 TensorFlow 但是觉得有点生硬不太好掌握，后来转了 PyTorch，很快就入门，自己慢慢能用 PyTorch 做些实验，通过这些实验也慢慢对一些常用模型有了更深的理解。</p>
<p>后来的实习经历主要使用 TensorFlow，所以又把 TensorFlow 捡了起来，由于有了经验，再次学习 TensorFlow 时，发现它其实并没有想象中的那么难，后来发现 TensorFlow 更倾向于<code>数学思维</code>, PyTorch 更倾向于 <code>工程思维</code>。</p>
<p>由于 PyTorch 的单机高效性，学习成本也比较低，近年来越来越多的论文用 PyTorch 实现；由于 TensorFlow 支持分布式，易部署所以业界用得比较多。</p>
<p>下文代码说明均使用 TensorFlow</p>
<h2 id="常用深度学习模块原理及应用">常用深度学习模块原理及应用</h2>
<h3 id="cnn">CNN</h3>
<p>CNN 主要由两个部分组成：卷积 (convolution) 和池化 (pooling) 两个部分。卷积主要用来提取特征；池化可以达到降维和降低模型复杂性抑制过拟合的目的，池化一般分为最大池化和平均池化，一般采用最大池化，因为<strong>最大池化具有平移不变性</strong>，这在一些任务中很重要。</p>
<h4 id="卷积">卷积</h4>
<p><img src="/images/posts/deeplearning/cnn.gif"></p>
<p>假设有一个 <span class="math inline">\(5\times 5\)</span> 的图像，使用一个 <span class="math inline">\(3\times 3\)</span> 的卷积核 (filter) 进行卷积，想得到一个 <span class="math inline">\(3\times 3\)</span> 的 特征表 (feature map)，如下所示：</p>
<p><img src="/images/posts/deeplearning/cnn01.png"></p>
<p><img src="/images/posts/deeplearning/cnn02.png"></p>
<p>卷积计算有三个重要的量：</p>
<ul>
<li>filter: 设置卷积核大小</li>
<li>padding: 设置输入的填充区域</li>
<li>stride: 设置卷积核窗口的滑动步数</li>
</ul>
<p>一个 <span class="math inline">\(n\times n\)</span> 的图像，经过大小为 <span class="math inline">\(f\times f\)</span> 的卷积核，填充 (padding) 大小为 <span class="math inline">\(p\)</span>，步数 (stride) 为 <span class="math inline">\(s\)</span>，则得到的 feature map 为</p>
<p><span class="math display">\[(\left \lfloor \frac{n+2p-f}{s} \right \rfloor + 1, \left \lfloor \frac{n+2p-f}{s} \right \rfloor + 1)\]</span></p>
<p>一般我们想得到<strong>输入等于输出</strong>时需要设置 padding 值</p>
<p><img src="/images/posts/deeplearning/cnn03.png"></p>
<p>一般我们想使得<strong>输出的尺寸比输入更低</strong>时，一般有两种方式：池化、调大 stride。通过调大 stride 的方式是改变卷积核的移动步长从而跳过一些像素</p>
<p><img src="/images/posts/deeplearning/cnn04.png"></p>
<p>为了能捕获到不同的特征边缘信息，一般会使用多卷积核</p>
<p><img src="/images/posts/deeplearning/cnn05.png"></p>
<h4 id="池化">池化</h4>
<p>一般对卷积后得到的 feature map 进行降维，池化的作用：</p>
<ul>
<li>降低模型复杂性，抑制过拟合</li>
<li>平移不变性(一般对于最大池化来说)</li>
</ul>
<blockquote>
<p><strong>平移不变性</strong> 如果人们选择图像中的连续范围作为池化区域，并且只是池化相同(重复)的隐藏单元产生的特征，那么，这些池化单元就具有平移不变性 (translation invariant)。这就意味着即使图像经历了一个小的平移之后，依然会产生相同的 (池化的) 特征。在很多任务中 (例如物体检测、声音识别)，我们都更希望得到具有平移不变性的特征，因为即使图像经过了平移，样例(图像)的标记仍然保持不变。</p>
</blockquote>
<p><img src="/images/posts/deeplearning/cnn06.gif"></p>
<h4 id="其他概念">其他概念</h4>
<h5 id="感受野">感受野</h5>
<p>feature map 中的输出对应回得到这个输出的输入区域，feature map 中的输出处于感受野的中心位置</p>
<h5 id="空洞膨胀卷积">空洞(膨胀)卷积</h5>
<p>卷积核中只有一部分起作用</p>
<p><img src="/images/posts/deeplearning/cnn07.gif"></p>
<h3 id="tensorflow-的封装">TensorFlow 的封装</h3>
<p>TensorFlow 支持一维卷积 <code>conv1d</code> 、二维卷积 <code>conv2d</code> 和三维卷积 <code>conv3d</code> 操作，用得比较多得是一维和二维，具体的函数为：</p>
<ul>
<li>conv1d
<ul>
<li><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv1d" target="_blank" rel="noopener">tf.nn.conv1d</a></li>
<li><a href="https://tensorflow.google.cn/api_docs/python/tf/layers/conv1d" target="_blank" rel="noopener">tf.layers.conv1d</a> : 相比 <code>tf.nn.conv1d</code> 提供了更多的操作接口</li>
</ul></li>
<li>conv2d
<ul>
<li><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">tf.nn.conv2d</a></li>
<li><a href="https://tensorflow.google.cn/api_docs/python/tf/layers/conv2d" target="_blank" rel="noopener">tf.layers.conv2d</a></li>
</ul></li>
</ul>
<p>一维卷积一般用于处理序列信息，常用于NLP任务，二维卷积一般用来处理三通道RGB的图片。</p>
<p>一般用一维卷积处理的任务可以通过扩充维度来使用二维卷积来处理，至于如何选择还得看具体的任务，NLP任务来说个人更偏向于使用 <code>conv1d</code></p>
<p>无论是一维还是二维，卷积神经网络都具有相同的特点和相同的处理方法，区别在于滤波器的对数据的滑动方式不同</p>
<p><img src="/images/posts/deeplearning/cnn08.png"></p>
<h3 id="rnn及其变体">RNN及其变体</h3>
<h4 id="最简单的神经网络模型">最简单的神经网络模型</h4>
<p>没有上下文记忆，只保存当前状态</p>
<p><img src="/images/posts/deeplearning/rnn01.png"></p>
<p><span class="math display">\[h = \sigma(Wx) \\\\
y = Vh\]</span></p>
<h4 id="rnn">RNN</h4>
<h5 id="原理">原理</h5>
<p>含有上下文状态的神经网络模型</p>
<p><img src="/images/posts/deeplearning/rnn02.png"></p>
<p><span class="math display">\[h_{t} = \sigma(Wx_{t} + Uh_{t-1}) \\\\
y_{t} = Vh_{t}\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(h_{t}\)</span> 是 t 时刻的隐藏状态值</li>
<li><span class="math inline">\(h_{t-1}\)</span> 是 t-1 时刻的隐藏状态值</li>
</ul>
<h5 id="缺点">缺点</h5>
<p>尽管 RNN 成功记忆了部分上下文信息，但存在一个很大的缺陷，那就是它很难记住长期的记忆，因为随着网络的加深网络会出现梯度弥散的问题，也就是<strong>很长的时刻前的输入，对现在的网络影响非常小，反向传播时那些梯度，也很难影响很早以前的输入</strong></p>
<h4 id="lstm-long-short-term-memory-长短时记忆">LSTM (Long Short-Term Memory) 长短时记忆</h4>
<p>为了克服原始 RNN 的缺点，一些带门控单元的 RNN 被提出来，LSTM 是其中一个。</p>
<h5 id="lstm-的工作原理">LSTM 的工作原理</h5>
<p><img src="/images/posts/deeplearning/rnn03.png"></p>
<p><span class="math display">\[\begin{align*}
f_{t} &amp; = \sigma(W_{f} [h_{t-1}, x_{t}] + b_{f}) \\\\
i_{t} &amp; = \sigma(W_{i} [h_{t-1}, x_{t}] + b_{i}) \\\\
o_{t} &amp; = \sigma(W_{o} [h_{t-1}, x_{t}] + b_{o}) \\\\
\widetilde{c_{t}} &amp; = tanh(W_{c} [h_{t-1}, x_{t}] + b_{c}) \\\\
c_{t} &amp; = f_{t}\odot c_{t-1} + i_{t} \odot \widetilde{c_{t}} \\\\
h_{t} &amp; = o_{t} \odot tanh(c_{t})
\end{align*}\]</span></p>
<p>LSTM 有三个门:</p>
<ul>
<li>input gate: 输入门</li>
<li>output gate: 输出门</li>
<li>forget gate: 遗忘门</li>
</ul>
<p>有两个重要的 state (或memory)：</p>
<ul>
<li>长期记忆 (long-term memory: ltm, 通常被称为cell state)</li>
<li>工作记忆 (working memory: wm, 通常被称为hidden state)</li>
</ul>
<p>LSTM 的迭代过程：</p>
<ul>
<li>选择性遗忘部分长期记忆：将记忆中不需要的记忆移除</li>
<li>将当前时刻的一些信息加入到长期记忆中
<ul>
<li>计算候选长期记忆</li>
<li>选择函数</li>
</ul></li>
<li>从长期记忆中提取工作记忆
<ul>
<li>计算候选的工作记忆</li>
<li>选择函数</li>
</ul></li>
</ul>
<p><strong>1. 选择性遗忘部分长期记忆</strong></p>
<p>这部分主要由遗忘门来控制，通过遗忘门来决定哪些长期记忆能够被留下，在 t 时刻有：</p>
<p><span class="math display">\[remember_{t} = \sigma(W_{r} x_{t} + U_{r}wm_{t-1})\]</span></p>
<p>这里的 <span class="math inline">\(remember_{t}\)</span> 是一个布尔序列，长度和上一时刻的长期记忆 <span class="math inline">\(ltm_{t-1}\)</span> 相同，<span class="math inline">\(remember_{t}\)</span> 中值为 <code>1</code> 代表保留 <span class="math inline">\(ltm_{t-1}\)</span> 对应位置的值，<code>0</code> 代表遗忘。</p>
<p>上述的公式最终表示成：</p>
<p><span class="math display">\[f_{t} = \sigma(W_{f} [h_{t-1}, x_{t}] + b_{f})\]</span></p>
<p><img src="/images/posts/deeplearning/rnn04.png"></p>
<p>则经过遗忘门后<strong>保留下来的记忆</strong>为：</p>
<p><span class="math display">\[old\_ltm_{t} = f_{t}\odot ltm_{t-1}\]</span></p>
<p><strong>2. 将当前时刻的一些信息加入到长期记忆中</strong></p>
<p>除了某些老的记忆需要保留，当前时刻的部分记忆也需要保留。</p>
<p>首先需要从上一时刻工作记忆及当前输入中计算全体候选记忆，在 t 时刻有：</p>
<p><span class="math display">\[ltm^{&#39;}_{t} = tanh(W_{l}x_{t} + U_{l}wm_{t-1})\]</span></p>
<p>这里的 <span class="math inline">\(ltm^{&#39;}\)</span> 代表可能加入长期记忆的记忆序列，长度和 <span class="math inline">\(ltm_{t-1}\)</span> 相同。</p>
<p>上述公式还可表示成：</p>
<p><span class="math display">\[\widetilde{c_{t}} = tanh(W_{c} [h_{t-1}, x_{t}] + b_{c})\]</span></p>
<p>有了候选记忆后，需要一个选择函数负责实际选择哪些记忆可以加入长期记忆，这个需要输入门来控制，在 t 时刻有：</p>
<p><span class="math display">\[save_{t} = \sigma(W_{s} x_{t} + U_{s}wm_{t-1})\]</span></p>
<p>上述公式还可表示成：</p>
<p><span class="math display">\[i_{t} = \sigma(W_{i} [h_{t-1}, x_{t}] + b_{i})\]</span></p>
<p>有了候选的长期记忆和选择函数后，我们就可以确定哪些记忆是要添加到长期记忆的。在 t 时刻：</p>
<p><span class="math display">\[new\_ltm_{t} = save_{t} \odot lsm^{&#39;}\]</span></p>
<p><img src="/images/posts/deeplearning/rnn04.png"></p>
<p>上面已经得到了老的长期记忆中在当前时刻要保留的部分 <span class="math inline">\(old\_ltm_{t}\)</span> ，以及当前时刻的候选记忆中要保留的部分 <span class="math inline">\(new\_ltm_{t}\)</span> ，因此 t 时刻最终确定的长期记忆为</p>
<p><span class="math display">\[ltm_{t} = old\_ltm_{t} + new\_ltm_{t}\]</span></p>
<p>上述公式还可表示成：</p>
<p><span class="math display">\[c_{t} = f_{t}\odot c_{t-1} + i_{t} \odot \widetilde{c_{t}}\]</span></p>
<p><img src="/images/posts/deeplearning/rnn06.png"></p>
<p><strong>3. 从长期记忆中提取工作记忆</strong></p>
<p>长期记忆需要应用在当前的工作记忆中才有作用。</p>
<p>从上一个工作记忆和当前输入中选择，通过输出门来控制，在 <span class="math inline">\(t\)</span> 时刻有</p>
<p><span class="math display">\[focus_{t} = \sigma(W_{f}x_{t} + U_{f}wm_{t-1})\]</span></p>
<p>上述公式可以表示成：</p>
<p><span class="math display">\[o_{t} = \sigma(W_{o} [h_{t-1}, x_{t}] + b_{o})\]</span></p>
<p>将长期记忆转换为工作记忆，在 t 时刻有</p>
<p><span class="math display">\[wm_{t}^{&#39;} = tanh(ltm_{t-1})\]</span></p>
<p>上述公式还可表示成：</p>
<p><span class="math display">\[\widetilde{h}_{t} = tanh(c_{t})\]</span></p>
<p>有了选择函数和候选记忆则 t 时刻最终的工作记忆有</p>
<p><span class="math display">\[wm_{t} = focus_{t} \odot wm_{t}^{&#39;}\]</span></p>
<p>上述公式还可表示成</p>
<p><span class="math display">\[\begin{align*}
h_{t} &amp; = o_{t} \odot \widetilde{h}_{t} \\\\ 
&amp; = o_{t} \odot tanh(c_{t})\end{align*}\]</span></p>
<p><img src="/images/posts/deeplearning/rnn07.png"></p>
<p>因此，LSTM 最终的结构为</p>
<p><img src="/images/posts/deeplearning/rnn03.png"></p>
<h5 id="lstm-的变种">LSTM 的变种</h5>
<p>上述讲的 LSTM 是经典的结构，LSTM 有不少变种，用得较多的由两种：</p>
<ul>
<li>Peephole LSTM</li>
<li>Gated Recurrent Unit (GRU)</li>
</ul>
<p><strong>1. Peephole LSTM</strong></p>
<p>普通的 LSTM 的所有的门的决策全部都是由输入 <span class="math inline">\(x\)</span> 和 工作记忆 <span class="math inline">\(h_{t-1}\)</span> 决定，Peephole LSTM 改进了门的实现，让长期记忆 <span class="math inline">\(ltm_{t-1}\)</span> 也参与门的决策。</p>
<p><img src="/images/posts/deeplearning/rnn08.png"></p>
<p><strong>2. Gated Recurrent Unit (GRU)</strong> 可以把 GRU 当作 LSTM 的高效压缩版，GRU 有两个门 reset gate 和 update gate，GRU使用了update gate替代了forget gate和input gate,而且将long-term memory 和 working memory 合并了，并做了一些细微的调整。</p>
<p><img src="/images/posts/deeplearning/rnn09.png"></p>
<h5 id="双向-rnn">双向 RNN</h5>
<p>双向的 RNN 同时考虑“过去”和“未来”的信息，可以更好的捕捉整体序列的语义信息、结构关系</p>
<p><img src="/images/posts/deeplearning/rnn10.png"></p>
<h3 id="attention机制的原理以及实现">Attention机制的原理以及实现</h3>
<p>Attention 目标是从众多信息中选择出对当前任务目标更关键的信息，一般通过加权的方式进行。</p>
<img src="/images/posts/deeplearning/attn02.png">
<p align="center">
Attention 在机器阅读理解任务中
</p>
<p>计算 Attention 一般需要三个量 Q (query), K (key), V (value)</p>
<p>K 和 V 一般是对应的，在 NLP 中一般 K = V，对于 self-attention 有 Q = K = V</p>
<p>Attention 的计算主要分三步:</p>
<p><span class="math display">\[S = similarity(Q, K) \\\\
\alpha = softmax(S) \\\\
V^{&#39;} = \alpha \cdot V\]</span></p>
<p>其中 <span class="math inline">\(similarity(*)\)</span> 计算的是 Q 和 K 的相似度，该函数可由简单的点乘或者 <span class="math inline">\(cosine\)</span> 函数来充当，也可使用多层感知机 (MLP) 来充当</p>
<h5 id="nlp-领域常见的-attention-模型">NLP 领域常见的 Attention 模型</h5>
<p><strong>1. multihead-attention</strong> 这是由 Google 17年在 <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">《Attention is all you need》</a> 这篇论文提出的，区别于一般的 attention，multihead-attention 在计算时融合了不同子空间的信息，它的结构如下</p>
<p><img src="/images/posts/deeplearning/attn01.png"></p>
<p><strong>2. BIDAF (Bidirectional Attention Flow)</strong> 16 年在论文《Bidirectional Attention Flow for Machine Comprehension》中提出， 是机器阅读理解中常用的 Attention 计算方式</p>
<p><img src="/images/posts/deeplearning/attn03.png"></p>
<h5 id="使用-tensorflow-实现-self-attention">使用 TensorFlow 实现 Self-Attention</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(value,</span></span></span><br><span class="line"><span class="function"><span class="params">              initializer=tf.truncated_normal_initializer<span class="params">(stddev=<span class="number">0.1</span>)</span>)</span>:</span></span><br><span class="line">    Q = value</span><br><span class="line">    K = value</span><br><span class="line">    V = value</span><br><span class="line"></span><br><span class="line">    shape = V.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    W = tf.get_variable(<span class="string">'attn_W'</span>, </span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], shape[<span class="number">-1</span>]],</span><br><span class="line">                        initializer=initializer)</span><br><span class="line"></span><br><span class="line">    U = tf.get_variable(<span class="string">'attn_U'</span>,</span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], shape[<span class="number">-1</span>]],</span><br><span class="line">                        initializer=initializer)</span><br><span class="line"></span><br><span class="line">    P = tf.get_variable(<span class="string">'attn_P'</span>,</span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], <span class="number">1</span>], </span><br><span class="line">                        initializer=initializer)</span><br><span class="line"></span><br><span class="line">    Q = tf.reshape(Q, [<span class="number">-1</span>, shape[<span class="number">-1</span>]])</span><br><span class="line">    K = tf.reshape(K, [<span class="number">-1</span>, shape[<span class="number">-1</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate similarity</span></span><br><span class="line">    S = tf.multiply(</span><br><span class="line">        tf.matmul(Q, W), </span><br><span class="line">        tf.matmul(K, U))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># normalize</span></span><br><span class="line">    alpha = tf.nn.softmax(</span><br><span class="line">        tf.reshape(tf.matmul(S, P), [<span class="number">-1</span>, shape[<span class="number">1</span>], <span class="number">1</span>]), axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply attention</span></span><br><span class="line">    V_attn = tf.multiply(alpha, V)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> V_attn, alpha</span><br></pre></td></tr></table></figure>
<h3 id="transformer">Transformer</h3>
<p>《Attention is all you need》还提出了基于 multihead-attention 的 transformer 结构，现在来看可以把 transformer 当成 CNN、RNN 同等重要的基本结构了。</p>
<p>近期热门的 GPT, BERT 都是基于 transformer 的，transformer 和 CNN 都可以支持并行计算，特征提取的能力也非常强，不过 transformer 也有缺点，它需要较大的参数量才能 work，所以当计算资源不是太强大时，transformer 往往不是很好的选择</p>
<p><img src="/images/posts/deeplearning/transformer01.png"></p>
<h2 id="常用的网络训练技巧">常用的网络训练技巧</h2>
<h3 id="正则化的意义及使用方式">正则化的意义及使用方式</h3>
<p>一般来说模型的风险可分为期望风险、经验风险和结构风险，损失函数是三者的基础，而讲到损失函数就不可避免的谈到成本函数、目标函数，所以借正则化这一节来区分这几个概念。</p>
<h4 id="损失函数">损失函数</h4>
<p>损失函数一般是指对单个样本做的损失，用来衡量对单个样本的预测和真实值之间的差距，损失函数一般可记为 <span class="math inline">\(L(y, f(x))\)</span> , <span class="math inline">\(f(x)\)</span> 代表预测值</p>
<p>常见的损失函数有：</p>
<ul>
<li>0-1 损失函数： <span class="math inline">\(L(y, f(x)) = \begin{cases} 1 &amp; \text{ , } y\neq f(x) \\\\ 0 &amp; \text{ , } y = f(x) \end{cases}\)</span></li>
<li>平方损失函数: <span class="math inline">\(L(y, f(x)) = (y- f(x))^{2}\)</span></li>
<li>绝对损失函数: <span class="math inline">\(L(y, f(x)) = |y- f(x)|\)</span></li>
<li>负对数似然: <span class="math inline">\(L(y, p(y|x)) = -log\ p(y|x)\)</span></li>
</ul>
<h4 id="经验风险-empirical-risk">经验风险 (empirical risk)</h4>
<p>计算训练集中所有样本的平均损失值，用这个值去衡量模型的能力，这就是经验风险</p>
<p><span class="math display">\[R_{emp} (f) = \frac{1}{N} \sum_{i=1}^{N} L(y_{i}, f(x_{i}))\]</span></p>
<p>所谓经验风险最小化就是让上述式子最小化，经验风险越小说明模型f(X)对训练集的拟合程度越好</p>
<h4 id="期望风险-expected-risk">期望风险 (expected risk)</h4>
<p>对于未知的样本数据的数量是不容易确定的，所以就没有办法和经验风险一样最小化所有样本损失函数的平均值，一般用期望来衡量未知样本的风险，称为期望风险。</p>
<p>假设X和Y服从联合分布 <span class="math inline">\(P(X,Y)\)</span> ，那么期望风险就可以表示为：</p>
<p><span class="math display">\[R_{exp}(f) = E_{p}[L(Y, f(X))] = \int_{x\times y} L(y, f(x)) P(x, y) dxdy\]</span></p>
<p>期望风险表示的是全局的概念，表示的是决策函数对所有的样本预测能力的大小，而经验风险则是局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。因为期望风险比较难求，实际情况下用得比较多的还是经验风险。</p>
<h4 id="结构风险-structual-risk">结构风险 (structual risk)</h4>
<p>如果只考虑经验风险那么模型很容易过拟合，导致模型的泛化能力差，这时候需要引入结构风险，结构风险是对经验风险和期望风险的折衷，结构风险一般可通过<strong>正则化</strong>来引入，这里点了本节的题，稍后会更详细介绍常见的正则化手段</p>
<p><span class="math display">\[R_{srm}(f) = \frac{1}{N} \sum_{i=1}^{N} L(y_{i}, f(x_{i})) + \lambda J(f)\]</span></p>
<h4 id="成本函数">成本函数</h4>
<p>一般指数据集上总的损失，为了降低结构风险，往往加上正则项，所以成本函数可记为</p>
<p><span class="math display">\[J(W, b) = \frac{1}{N} \sum_{i=1}^{N} L(y_{i}, \hat{y_{i}})) + \lambda J(w)\]</span></p>
<h4 id="目标函数">目标函数</h4>
<p>目标函数是一个非常广泛的名称，一般我们先确定一个“目标函数”再去优化它，不同的任务中“目标函数”可以是：</p>
<ul>
<li>最小化平方差错误成本函数 (CART, 线性回归等任务)</li>
<li>最大化log-相似度或最小化信息熵损失函数</li>
<li>最小化 hinge 损失函数 (SVM)</li>
</ul>
<h4 id="正则化">正则化</h4>
<p>模型越复杂时包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。</p>
<p>可以简单理解为模型的复杂程度是过拟合的重要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度，所以我们一般通过正则化的方式去惩罚参数。</p>
<p>常用的正则化的方式有：</p>
<ul>
<li>L1 正则</li>
<li>L2 正则</li>
<li>随机失活 dropout</li>
</ul>
<h5 id="l1-正则-l2正则">L1 正则， L2正则</h5>
<p>L1 正则使用 L1 范数，倾向于把参数变稀疏；L2 正则使用 L2 范数，倾向于把参数变小；一般来说使用 L2 正则居多</p>
<blockquote>
<p><strong>范数</strong> 是具有“长度”概念的函数。在线性代数、泛函分析及相关的数学领域，是一个函数，其为向量空间内的所有向量赋予非零的正长度或大小</p>
</blockquote>
<p>在图像上两者的区别</p>
<p><img src="/images/posts/deeplearning/norm01.png"></p>
<p>更一般的，在等高线图上有</p>
<p><img src="/images/posts/deeplearning/norm02.png"></p>
<p>可以想到加上了 L1正则后损失函数的等高线和 L1正则图像 的交点集中在 (0, 1) , (1, 0) 等稀疏点，因此 L1 正则倾向于将参数稀疏化，而 L2 正则倾向于将参数变小。</p>
<h5 id="dropout">dropout</h5>
<p>随机失活 dropout 一般用于神经网络模型，它的原理是对于神经网络单元，按照一定的概率将其<strong>暂时</strong>从网络中丢弃。对于随机梯度下降来说，由于是随机丢弃，故而每一个 mini-batch 都能用不同的神经元训练不同的网络。</p>
<p><img src="/images/posts/deeplearning/norm03.png"></p>
<p>dropout 的实现方式非常简单，以下是用 numpy 来实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob=<span class="number">0.2</span>)</span></span></span><br><span class="line">    shape = list(x.shape)</span><br><span class="line">    mask = np.random.rand(shape[<span class="number">0</span>], shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">    x = x * mask</span><br><span class="line">    output = x / keep_prob</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="规范化的作用及常用的规范化方式">规范化的作用及常用的规范化方式</h3>
<p>规范化一般指标准规范化，将数据按比例缩放，使之落入一个小的特定区间。规范化可以在一定程度上抵制异常值 (outliers) 的影响，使得网络更稳定，更容易找到最优解</p>
<p>未归一化的等高线图</p>
<p><img src="/images/posts/deeplearning/bn01.png"></p>
<p>归一化的等高线图</p>
<p><img src="/images/posts/deeplearning/bn02.png"></p>
<p>常见的归一化方式有：</p>
<ul>
<li>Batch Normalization</li>
<li>Layer Normalization</li>
<li>Group Normalization</li>
<li>Instance Normalization</li>
</ul>
<p>它们的原理都是</p>
<p><span class="math display">\[a^{*} = \frac{a-\mu }{\sigma} \\\\
a^{norm} = \eta a^{*} + \beta\]</span></p>
<p>其中 <span class="math inline">\(a^{*}\)</span> 是标准归一化的结果， <span class="math inline">\(\eta\)</span> 和 <span class="math inline">\(\beta\)</span> 是网络自动学习的参数，它们的作用是对 <span class="math inline">\(a^{*}\)</span> 进行放缩移位</p>
<p>它们的区别在于对统计量 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma\)</span> 的求解，下图显示了各个 Normalization 使用不同的单元去计算两个统计量</p>
<p><img src="/images/posts/deeplearning/bn03.png"></p>
<p><strong>使用经验</strong></p>
<ul>
<li>Batch Normalization: 一般用于 CNN 和 MLP，不用于 RNN，实践证明一般用在激活函数后</li>
<li>Layer Normalization: 可用于 RNN，其他情况下不如 Batch Normalization</li>
<li>Group Normalization: 一般用于 CNN</li>
<li>Instance Normalization: 不常用</li>
</ul>
<h3 id="常见的激活函数及选择">常见的激活函数及选择</h3>
<p>激活函数是非线性的，一般是全局可导的，常见的激活函数有</p>
<ul>
<li>sigmoid</li>
<li>tanh</li>
<li>relu (Rectified Linear Unit) 线性修正单元
<ul>
<li>leaky_relu</li>
<li>gelu</li>
<li>elu</li>
</ul></li>
</ul>
<h4 id="sigmoid">sigmoid</h4>
<p><span class="math display">\[\sigma(z) = \frac{1}{1+e^{-z}}\]</span></p>
<p><img src="/images/posts/deeplearning/act01.png"></p>
<p>sigmoid 一般用于二分类任务</p>
<h4 id="tanh">tanh</h4>
<p><span class="math display">\[tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\]</span></p>
<p><img src="/images/posts/deeplearning/act02.png"></p>
<p>tanh 一般和 RNN 结合使用</p>
<h4 id="relu">relu</h4>
<p><span class="math display">\[ReLU(z) = max(0, z) = \begin{cases}
0 &amp; \text{ , } x\leqslant 0 \\\\ 
z &amp; \text{ , } x &gt; 0 
\end{cases}\]</span></p>
<p><img src="/images/posts/deeplearning/act03.png"></p>
<p>对比三者的图像可知，sigmoid 和 tanh 只有很小部分区域的梯度变化比较明显，大部分区域梯度几乎没有变化，正是这个原因很容易导致梯度弥散问题。而 ReLU 没有这个缺点，所以 ReLU 广泛用做隐层的激活函数</p>
<h3 id="常见的优化器及选择">常见的优化器及选择</h3>
<p>深度学习中一般使用梯度下降算法来优化，常用的优化器有</p>
<ul>
<li>随机梯度下降 (Stochastic Gradient Decent, SGD)</li>
<li>动量梯度下降 (Gradient Descent with Momentum)</li>
<li>均方根支梯度下降 (Root Mean Square Prop, RMSProp)</li>
<li>自适应矩估计 (Adaptive Moment Estimation, Adam)</li>
</ul>
<p><img src="/images/posts/deeplearning/loss01.gif"></p>
<h4 id="sgd">SGD</h4>
<p>每次通过一个样本来迭代更新</p>
<p><span class="math display">\[\begin{align*}
J(w, b) = L (\hat{y^{(i)}}, y^{(i)}) + \frac{\lambda }{2}\sum \left \| w \right \|_{F}^{2} \\\\
w_{j} := w_{j} - \alpha \frac{\partial J(w, b)}{\partial w_{j}} \\\\
b_{j} := b_{j} - \alpha \frac{\partial J(w, b)}{\partial b_{j}}
\end{align*}\]</span></p>
<p><img src="/images/posts/deeplearning/loss02.png"></p>
<h4 id="momentum">Momentum</h4>
<p>计算梯度的指数加权平均数，并利用该值来更新参数值</p>
<p><span class="math display">\[\begin{align*}
&amp; \upsilon_{dw} = \beta \upsilon_{dw} + (1-\beta) dw \\\\
&amp; \upsilon_{db} = \beta \upsilon_{db} + (1-\beta) db \\\\
&amp; w := w - \alpha \upsilon_{dw} \\\\
&amp; b := b - \alpha \upsilon_{db}
\end{align*}\]</span></p>
<p>SGD 在局部沟壑中很容易发生振荡，所以在这种情况下下降速度会很慢，而动量能在一定程度上抑制这种震荡，使得SGD的下降更平稳</p>
<h4 id="rmsprop">RMSProp</h4>
<p>在梯度进行指数加权平均的基础上引入了平方和平方根</p>
<p><span class="math display">\[\begin{align*}
&amp; S_{dw} = \beta S_{dw} + (1-\beta) dw^{2} \\\\
&amp; S_{db} = \beta S_{db} + (1-\beta) db^{2} \\\\
&amp; w := w - \alpha \frac{dw}{\sqrt{S_{dw} + \epsilon }} \\\\
&amp; b := b - \alpha \frac{dw}{\sqrt{S_{db} + \epsilon }} \\\\
\tag{10}
\end{align*}\]</span></p>
<p><span class="math inline">\(\epsilon\)</span> 一般值很小，主要是用来提高数值稳定性，防止分母过小</p>
<p><strong>特点：</strong> 当 <span class="math inline">\(dw\)</span> 或 <span class="math inline">\(db\)</span> 较大时，<span class="math inline">\(dw^{2}\)</span> 和 <span class="math inline">\(db^{2}\)</span> 也会较大，因此 <span class="math inline">\(S_{dw}\)</span> <span class="math inline">\(S_{db}\)</span> 也是较大的，最终使得 <span class="math inline">\(\frac{dw}{\sqrt{S_{dw} + \epsilon}}\)</span> <span class="math inline">\(\frac{db}{\sqrt{S_{db} + \epsilon}}\)</span> 较小，这也减少了振荡</p>
<h4 id="adam">Adam</h4>
<p>可以认为是 <code>Momentum</code> 和 <code>RMSProp</code> 的结合</p>
<p><span class="math display">\[\begin{align*}
&amp; \upsilon_{dw} = \beta_{1} \upsilon_{dw} + (1-\beta _{1}) dw, \upsilon _{db} = \beta_{1} \upsilon_{db} + (1-\beta _{1}) db \\\\
&amp; S_{dw} = \beta_{2} S_{dw} + (1-\beta _{2}) dw^{2}, S_{db} = \beta_{2} S_{db} + (1-\beta _{2}) db^{2} \\\\
&amp; \upsilon_{dw}^{correct} = \frac{\upsilon _{dw}}{1-\beta_{1}^{t}}, \upsilon_{db}^{correct} = \frac{\upsilon_{db}}{1-\beta_{1}^{t}} \\\\
&amp; S_{dw}^{correct} = \frac{S_{dw}}{1-\beta_{2}^{t}}, S_{db}^{correct} = \frac{S_{db}}{1-\beta_{2}^{t}} \\\\
&amp; w := w - \alpha \frac{\upsilon_{dw}^{correct}}{\sqrt{S_{dw}^{correct}} + \epsilon} \\\\
&amp; b := b - \alpha \frac{\upsilon_{db}^{correct}}{\sqrt{S_{db}^{correct}} + \epsilon} \\\\
\tag{11}
\end{align*}\]</span></p>
<p><span class="math inline">\(\beta _{1}\)</span>为第一阶矩，<span class="math inline">\(\beta _{2}\)</span> 为第二阶矩</p>
<h4 id="使用经验">使用经验</h4>
<p>一般来说 Adam, Adamax, Adadelta, Adagrad, AdamW 等优化算法会更快更暴力的收敛，但是往往不易找到全局最优，经验上来说一般使用动量梯度下降更容易找到全局最优，不过收敛速度慢</p>
<h3 id="mask-的作用">mask 的作用</h3>
<p>mask 是网络训练的一种常用的 trick, 在 NLP 任务中训练数据通常以 mini-batch 的方式准备,一般来说 mini-batch 中的数据是长短不一的，所以往往需要对数据进行 padding，使得 mini-batch 长短一致。</p>
<p><img src="/images/posts/deeplearning/mask01.png"></p>
<p>为了减弱 padding 部分的影响，一般对 mini-batch 进行 mask, 然后减小 padding 部分的权值</p>
<p><img src="/images/posts/deeplearning/mask02.png"></p>
<p>对于抽取性的任务，如：机器阅读理解，文本摘要，使用 mask 一般会使模型效果有略微提升</p>
<h4 id="tensorflow-实现-mask">TensorFlow 实现 mask</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_seq = tf.placeholder(tf.int32, </span><br><span class="line">                           [batch_size, <span class="keyword">None</span>],</span><br><span class="line">                           name=<span class="string">"sequence"</span>)</span><br><span class="line"><span class="comment"># TODO ...</span></span><br><span class="line"></span><br><span class="line">mask_value = <span class="number">1e-12</span></span><br><span class="line">seq_mask = tf.cast(</span><br><span class="line">              tf.cast(batch_seq, tf.bool), tf.float32)</span><br><span class="line">masked = batch_seq * seq_mask + mask_value * (<span class="number">1.</span> - seq_mask)</span><br></pre></td></tr></table></figure>
<h3 id="模型效果提升偏方">模型效果提升偏方</h3>
<h4 id="迁移学习">迁移学习</h4>
<p>在 NLP 中迁移学习的思想一般是使用预训练词向量(如 word2vec、fasttext、glove)或者使用预训练语言模型 (ELMo、GPT、BERT)</p>
<h5 id="预训练词向量">预训练词向量</h5>
<p>由于向量表示是低维稠密的分布式向量，所以可以进行语义计算，但只是对词做了简单的空间映射，也就是说同一个词的向量表示是相同的。使用的话，一般将预训练的词向量权值表替换掉随机初始化的权值表，在 TensorFlow 实现可以是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pretrained_word_matrix = tf.get_variable(</span><br><span class="line">    <span class="string">'word_embeddings'</span>,</span><br><span class="line">    shape=[vocab_size, embedding_size],</span><br><span class="line">    initializer=tf.constant_initializer(</span><br><span class="line">                    pretrained_word_embeddings),</span><br><span class="line">    trainable=<span class="keyword">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h5 id="预训练语言模型">预训练语言模型</h5>
<p>今年 (2018年) 预训练语言模型陆陆续续被提出, ELMo、GPT、BERT 是之中的三个代表，其中 BERT 的推出更是 (绝对地) 刷新了多项 NLP 任务 (一般来说 Google 一出手，其他人就没法做了)。</p>
<p>预训练语言模型和预训练词向量的区别在于可以认为预训练语言模型训练的是模型 <span class="math inline">\(f(x)\)</span> ，预训练语言模型可以学到上下文信息，根据输入上下文的不同可以动态的得到不同的词向量。</p>
<p>比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我喜欢吃 苹果 </span><br><span class="line">我喜欢用 苹果 电脑</span><br></pre></td></tr></table></figure>
<p>预训练词向量中两个句子中的<strong>苹果</strong>的词向量是相同的，而预训练语言模型得到的<strong>苹果</strong>是不同的，因为两个句子的上下文是不同的</p>
<p>关于 ELMo、GPT、BERT 的区别在下一节再做简要介绍</p>
<h4 id="鉴别学习-identity-learning">鉴别学习 (identity learning)</h4>
<p>其实这一块我并没有深入的研究，只是在实习中用到了相关的知识。</p>
<p>不同的 NLP 任务往往有不同的指标，而指标往往是评估模型的重要因素，我们往往希望指标朝着期望的方向发展，所以我们可以把指标加入到损失函数中让优化器也一同优化指标，当然怎么加、如何控制是一门学问，这个还得凭自己经验来添加。</p>
<h4 id="模型融合-ensemble">模型融合 (ensemble)</h4>
<p>融合是一门大学问，一般来说可分为：</p>
<ul>
<li>单模型融合</li>
<li>多模型融合</li>
</ul>
<p>单模型融合一般通过 k-fold 交叉验证的方式进行</p>
<p><img src="/images/posts/deeplearning/ensemble01.png"></p>
<p align="center">
来自：《花式自然语言处理》，苏剑林，中山大学
</p>
<p>多模型融合一般也是通过 k-fold 交叉验证的方式：假如有 <span class="math inline">\(m\)</span> 种不同的模型，每种模型做 <span class="math inline">\(n\)</span> 划分交叉验证，可以得到 <span class="math inline">\(m \times n\)</span> 个不同的模型，通过一个新模型来融合这 <span class="math inline">\(m\times n\)</span> 个模型</p>
<p><img src="/images/posts/deeplearning/ensemble02.png"></p>
<p align="center">
来自：《花式自然语言处理》，苏剑林，中山大学
</p>
<h4 id="模型蒸馏">模型蒸馏</h4>
<p>模型蒸馏是 G.Hinton 在 2015 年提出来的 (来自论文：《Distilling the Knowledge in a Neural Network》), 可以把它当作一种模型压缩方法、迁移学习、模型融合方法。</p>
<p>它的主要思想是：通过训练一个（或多个）复杂的网络模型 (teacher)，然后用这个复杂模型去调教一个简单网络模型 (student) ，通过训练,简单模型会慢慢学到复杂模型的知识，甚至青出于蓝胜于蓝。</p>
<p>一般通过对 softmax 函数添加温度(temperature， T) 来实现</p>
<p><span class="math display">\[q_{i} = \frac{exp(z_{i} / T)}{\sum_{j} exp(z_{j}/T)}\]</span></p>
<p><img src="/images/posts/deeplearning/distilling01.png"></p>
<p>通过温度控制会使得 softmax 更稳定更<code>软</code>，最终有可能超过原模型。</p>
<h2 id="深度学习与nlp">深度学习与NLP</h2>
<h3 id="word2vec-vs-fasttext">word2vec vs fasttext</h3>
<p>两者都是得到低维稠密的分布式向量的方法，分布式向量表示具有语义计算、语义推理的能力</p>
<p><img src="/images/posts/deeplearning/word2vec01.png"></p>
<h4 id="word2vec">word2vec</h4>
<p>word2vec 是一种三层结构：输入层，投射层，输出层。</p>
<p><img src="/images/posts/deeplearning/word2vec02.png"></p>
<h5 id="输入层">输入层</h5>
<p>输入层是输入词的 one-hot 编码。</p>
<blockquote>
<p>深度学习框架处理 NLP 任务时，输入一般采用压缩 one-hot编码（字典表示），深度学习框架会自动转化为 one-hot 矩阵</p>
</blockquote>
<p><img src="/images/posts/deeplearning/word2vec07.png"></p>
<h5 id="投射层">投射层</h5>
<p>投射层即要训练的 embedding 层，训练完毕后我们可以用这一层得到的权值作为 word embedding 去训练模型。</p>
<h5 id="输出层">输出层</h5>
<p>输出层是 softmax 分类器，word2vec 采用了树结构表示的 hierarchical softmax 和构造正负样本的 negative sampling 方式去优化输出层，使得输出层的 softmax 函数不必轮询每个词，大大减小了计算路径的长度，从而保证 word2vec 的高效性。</p>
<h5 id="语料构造方式">语料构造方式</h5>
<p>虽然表面上 word2vec 是无监督的，但实际的训练过程是有监督的，它的<code>有监督语料</code>主要受两种模型影响</p>
<ul>
<li>CBOW (Continuous Bag of Words)</li>
<li>Skip-Gram</li>
</ul>
<p><img src="/images/posts/deeplearning/word2vec03.png"></p>
<h4 id="fasttext">fasttext</h4>
<p>fasttext 和 word2vec 原理类似，训练方式类似，区别在于：</p>
<ul>
<li>fasttext 需要提供有监督的语料</li>
<li>fasttext 使用了 n-gram 特征</li>
</ul>
<p><img src="/images/posts/deeplearning/word2vec04.png"></p>
<p><strong>n-gram 特征的主要作用</strong>： 通过共享权值，改善生成词向量的质量</p>
<h3 id="如何给网络添加特征">如何给网络添加特征</h3>
<p>特征有很多种，词、字、词性、位置信息、偏旁部首、n-gram等，一般来说添加适量的特征可以提升模型效果，特别在小数据量的情况下。</p>
<p>本人经验，添加特征的方式一般有两种：</p>
<ul>
<li>embedding: 将不同特征的 embedding 拼接作为最终的词嵌入向量表示</li>
<li>attention: 把特征当作 attention 里的 K (Key)</li>
</ul>
<h3 id="elmogptbert">ELMo、GPT、BERT</h3>
<p>这三个模型在今年陆陆续续地刷新了各大 NLP 榜单，总结了一下，它们的都有一个特点：<strong>深</strong>层网络 + <strong>大</strong>规模训练语料 + <strong>微调</strong>即可迁移到其他下游任务</p>
<ul>
<li>ELMo: 来自 Allen Institue, 论文 《Deep contextualized word representations》</li>
<li>GPT: 来自 OpenAI , 论文 《Improving Language Understanding by Generative Pre-Training》</li>
<li>BERT: 来自 Google , 论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</li>
</ul>
<p><img src="/images/posts/deeplearning/word2vec05.png"></p>
<p>ELMo 一般采用双向的 LSTM 或者 CNN 来训练，而 GPT、BERT 采用的是 transformer</p>
<h4 id="word2vecelmogptbert四者的关系">Word2vec、ELMo、GPT、BERT四者的关系</h4>
<p><img src="/images/posts/deeplearning/word2vec06.png"></p>
<p><strong>BERT</strong>的 transformer 之所以是双向的，是因为它构造训练语料的时候采用了一种 <code>next sentence prediction</code> 策略，详细请查阅原论文</p>
<h2 id="tensorflow-实战以dpcnn为例讲述如何实现一篇论文">Tensorflow 实战：以DPCNN为例讲述如何实现一篇论文</h2>
<p>本人经验，实现论文常用步骤</p>
<ul>
<li>首要条件是读懂论文，知道论文提出的模型采用了哪些结构，最好是看到公式心里就出现代码（比如说看到一堆 lstm 的公式就想起 lstm 相关的代码，这样可能一行代码就可以解决论文中一堆公式了）</li>
<li>语料如何构造</li>
<li>使用了什么 Attention 机制，以及如何计算 Attention</li>
<li>模型实现的 tricks 及调参说明 (最终能不能复现这部分很重要)</li>
</ul>
<p>本节以论文 <a href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf" target="_blank" rel="noopener">DPCNN: Deep Pyramid Convolutional Neural Networks for Text Categorization</a> 为例讲述如何用 TensorFlow 实现一篇论文</p>
<h3 id="读懂论文">1. 读懂论文</h3>
<p>DPCNN 是腾讯 AI Lab提出的，被ACL 2017 收录，主要用于文本分类，是一种基于 CNN 的浅层（相比大多数残差网络来说有点浅）残差网络结构</p>
<h3 id="实现模型结构">2. 实现模型结构</h3>
<p>大多数论文都会给出模型的网络结构，以及网络结构的说明，要实现论文这一块内容非常重要。</p>
<p>DPCNN 的模型结构为：</p>
<p><img src="/images/posts/deeplearning/dpcnn01.png"></p>
<p>可以看到这是一个残差结构，而且网络结构中的模块以卷积为主，那接下来的工作就是<code>搭乐高积木</code>的过程了</p>
<blockquote>
<p><strong>残差 = 预测值 - 观测值</strong> 神经网络在反向传播中会不断的更新梯度，当网络层数加深时，梯度在逐层传播的过程中会逐渐减弱，最后导致对先前的网络权重(由于采用了链式求导法则，所以必定要更新先前网络的权重)更新困难。<strong>在残差网络中，会在当前时刻的网络层中加入先前添加的观测连接（也称short connections)</strong>这样直接为当前层网络提供了一个直接连接先前层网络的通道使得可以直接更新先前网络的参数，因此缓解了梯度减小的问题</p>
</blockquote>
<p><strong>1. 首先定义好输入层的变量</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入的 mini-batch 数据</span></span><br><span class="line">self.inputs = tf.placeholder(tf.int32, </span><br><span class="line">                            [<span class="keyword">None</span>, self.config.max_sent_len],</span><br><span class="line">                            name=<span class="string">"inputs"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">self.labels = tf.placeholder(tf.int32,</span><br><span class="line">                            [<span class="keyword">None</span>], </span><br><span class="line">                            name=<span class="string">"label"</span>)</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> others</span></span><br></pre></td></tr></table></figure></p>
<p><strong>2. 词嵌入层</strong> 将压缩的 ont-hot 编码 (字典形式) 转为分布式编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_matrix = tf.get_variable(<span class="string">'word_embeddings'</span>,</span><br><span class="line">                           shape=[vocab_size(), embedding_size],</span><br><span class="line">                           trainable=<span class="keyword">False</span>)</span><br><span class="line">self.input_embedding = tf.nn.embedding_lookup(word_matrix, self.inputs)</span><br></pre></td></tr></table></figure>
<p><strong>3. 构造网络</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">inputs = self.input_embedding</span><br><span class="line"></span><br><span class="line">conv1 = tf.layers.conv1d(inputs, </span><br><span class="line">                         <span class="number">250</span>,    <span class="comment"># filters</span></span><br><span class="line">                         <span class="number">3</span>,      <span class="comment"># kernel size</span></span><br><span class="line">                         activation=tf.nn.relu,</span><br><span class="line">                         name=<span class="string">'conv1'</span>)</span><br><span class="line"></span><br><span class="line">conv2 = tf.layers.conv1d(conv1, </span><br><span class="line">                         <span class="number">250</span>,    <span class="comment"># filters</span></span><br><span class="line">                         <span class="number">3</span>,      <span class="comment"># kernel size</span></span><br><span class="line">                         activation=tf.nn.relu,</span><br><span class="line">                         name=<span class="string">'conv2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># short-connection (引入观测点)</span></span><br><span class="line">inputs = conv2 + inputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Repeat</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.config.num_blocks):</span><br><span class="line">    seq_len = inputs.get_shape()[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Downsampling: pooling / 2</span></span><br><span class="line">    poolings = tf.transpose(</span><br><span class="line">        tf.nn.top_k(tf.transpose(inputs, [<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]), k=seq_len // <span class="number">2</span>)[<span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    conv1 = tf.layers.conv1d(poolings, </span><br><span class="line">                             <span class="number">250</span>,    <span class="comment"># filters</span></span><br><span class="line">                             <span class="number">3</span>,      <span class="comment"># kernel size</span></span><br><span class="line">                             activation=tf.nn.relu,</span><br><span class="line">                             name=<span class="string">'conv1-%d'</span> % i)</span><br><span class="line">                             </span><br><span class="line">    conv2 = tf.layers.conv1d(conv1, </span><br><span class="line">                             <span class="number">250</span>,    <span class="comment"># filters</span></span><br><span class="line">                             <span class="number">3</span>,      <span class="comment"># kernel size</span></span><br><span class="line">                             activation=tf.nn.relu,</span><br><span class="line">                             name=<span class="string">'conv2-%d'</span> % i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># short-connection (引入观测点)</span></span><br><span class="line">    inputs = conv2 + poolings</span><br><span class="line"></span><br><span class="line"><span class="comment"># pooling</span></span><br><span class="line">self.outputs = tf.reduce_max(inputs, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="完善模型加入-tricks-调参数">3. 完善模型，加入 tricks, 调参数</h3>
<h3 id="本节后记">本节后记</h3>
<p>本节涉及到的完整代码已经放在 <a href="https://github.com/SeanLee97/clfzoo" target="_blank" rel="noopener">github / clfzoo</a> 上，欢迎给这个 repo 添砖加瓦</p>
<p>代码能力其实就一句话：无他，但手熟尔。</p>
<p>一定要向别人学习，学习阶段要多动手自己造轮子。</p>
<h2 id="写在最后">写在最后</h2>
<p>本人才疏学浅、思维深度浅加之表达能力几乎为 0，所以不免有所遗漏和错误的地方，欢迎大家指正，多交流！</p>
<h2 id="refrence">Refrence</h2>
<ul>
<li><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Neural_Networks_and_Deep_Learning/浅层神经网络" target="_blank" rel="noopener">浅层神经网络</a></li>
<li><a href="https://yq.aliyun.com/articles/617314" target="_blank" rel="noopener">直观理解深度学习的卷积操作，超赞！</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96" target="_blank" rel="noopener">池化</a></li>
<li>部分动图来自 https://github.com/vdumoulin/conv_arithmetic</li>
<li><a href="https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf" target="_blank" rel="noopener">Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding-LSTMs</a></li>
<li><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></li>
<li><a href="https://seanlee97.github.io/2018/10/01/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">常用的梯度下降优化算法</a></li>
<li><a href="https://blog.csdn.net/liyajuan521/article/details/44565269" target="_blank" rel="noopener">期望风险、经验风险与结构风险之间的关系</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/49323974" target="_blank" rel="noopener">谈谈损失函数, 成本函数, 目标函数 的区别</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27627299" target="_blank" rel="noopener">为什么要对数据进行归一化处理？</a></li>
<li><a href="https://kexue.fm/archives/4823" target="_blank" rel="noopener">分享一个slide：花式自然语言处理</a></li>
<li><a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></li>
<li><a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a></li>
<li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li>
<li><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li>
<li><a href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf" target="_blank" rel="noopener">Deep Pyramid Convolutional Neural Networks for Text Categorization</a></li>
<li><a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener">Bidirectional Attention Flow for Machine Comprehension</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/10/30/浅谈LDA主题模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/30/浅谈LDA主题模型/" itemprop="url">浅谈LDA主题模型(原理篇)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-30T21:55:00+08:00">
                2018-10-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/30/浅谈LDA主题模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/30/浅谈LDA主题模型/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>首先声明，这里的 LDA 是指 Latent Dirichlet Allocation 隐含狄利克雷分布，而不是 Linear Discriminant Analysis 线性判别分析 (笔者有幸在 City University of HK 听过一堂机器学习课，里面讲到了线性判别，受益匪浅，有机会再做分享)</p>
<p>除了看原论文 <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" target="_blank" rel="noopener">Latent Dirichlet Allocation</a> , 虽然论文一作不是安德鲁(吴恩达)，但是看到他的名字就知道这篇论文的水平，我也看了很多博客，写得最好的是<code>火光摇曳</code>的 《LDA数学八卦》，本文部分知识来自此篇博客。</p>
<h2 id="lda-的解释">LDA 的解释</h2>
<p>既然 LDA 是主题模型，那么它的功能应该是和 LSA、 pLSA 等差不多的。它们都可以得到文档的主题，以及主题词。LSA 一般通过 SVD 求解（可参考先前的文章 <a href="https://seanlee97.github.io/2018/09/01/SVD%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8ALSA%E7%9A%84%E6%B1%82%E8%A7%A3/">SVD的原理及LSA的求解</a>），而 pLSA 和 LDA 是概率模型。</p>
<p>那么什么是LDA呢？</p>
<p>假设有 M 篇文章，我们假设每篇文章中隐含有 K 个主题。其中文章和文章的词是可见的，而主题是假设出来的，且主题往往是受词影响的，可以把这些主题称为 <code>latent topics</code> 隐主题。</p>
<p><img src="/images/posts/lda_topic/topic-words.png"></p>
<p>假设每篇文章的长度为 N (即是每篇文章有 N 个词)，每篇文章都有各自的主题分布（是软分布），主题分布是<code>多项分布</code>， 该多项分布的参数服从<code>Dirichlet 分布</code>, 该<code>Dirichlet 分布</code>的参数为 <span class="math inline">\(\alpha\)</span></p>
<p>每个主题都有各自的词分布，词分布也是<code>多项分布</code>，其多项分布的参数也同样服从<code>Dirichlet 分布</code>，该<code>Dirichlet 分布</code>的参数为 <span class="math inline">\(\beta\)</span></p>
<p>通俗地说就是对于某篇文章中的第 n 个词，首先要从这篇文章的主题分布中采样一个主题，然后再在这个主题对应的词分布中采样一个词，然后不断重复这个过程，直到 m 篇文章全部完成上述过程。</p>
<p>而 LDA 是基于贝叶斯模型的，是一个三层的网络，贝叶斯模型有三个主要部分：先验分布、数据似然、后验分布，它们的关系为</p>
<p align="center">
先验分布 + 数据似然 = 后验分布
</p>
<p><strong>为了让上述关系满足递推关系</strong>，LDA 一般采用的分布是满足共轭分布的，共轭分布的先验分布和后验分布满足相同的分布律，<strong>即当前的求得的后验分布可作为下一次的先验分布</strong>。</p>
<p>上面内容先介绍了 LDA 的主要思想，下面主要介绍 LDA 的细节</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="gamma-函数">Gamma 函数</h3>
<p><code>Beta 分布</code> 和 <code>Dirichlet 分布</code> 中都用到 Gamma 函数 <span class="math inline">\(\Gamma(\alpha)\)</span> ,它的定义为：</p>
<p><span class="math display">\[\begin{align*}
&amp; \Gamma(\alpha) = \int_{0}^{+\infty } x^{\alpha-1} e^{-x} dx \\
&amp; where\ \ x &gt; 0
\end{align*} \tag{1}\]</span></p>
<p>另 <span class="math inline">\(x = t^{2}\)</span> 带回 <span class="math inline">\((1)\)</span> 式可得到 Gamma 函数的另一表达形式</p>
<p><span class="math display">\[\begin{align*}
&amp; \Gamma(\alpha) = 2 \int_{0}^{+ \infty} t^{2\alpha-1} e^{-t^{2}} dt \\
&amp; where\ \ t &gt; 0
\end{align*} \tag{2}\]</span></p>
<p>对于 <span class="math inline">\((1)\)</span> 式，将 <span class="math inline">\(\alpha = 1\)</span> 带入求积分易求得:</p>
<p><span class="math display">\[\Gamma(1) = 1 \tag{3}\]</span></p>
<p>对于 <span class="math inline">\((2)\)</span> 式，将 <span class="math inline">\(\alpha = \frac{1}{2}\)</span> 带入求积分可得：</p>
<p><span class="math display">\[\Gamma(\frac{1}{2}) = \sqrt{\pi} \tag{4}\]</span></p>
<p>接下来我们用分部积分法求解可得：</p>
<p><span class="math display">\[\begin{align*}
\Gamma(\alpha + 1) &amp; = \int _{0}^{+\infty} x^{\alpha} e^{-x} dx \\
&amp; = -\int _{0}^{+\infty} x^{\alpha}d e^{-x} \\
&amp; = -x^{\alpha} e^{-x} | _{0}^{+\infty} + \int _{0}^{+\infty} e^{-x}\alpha x^{\alpha-1} dx \\
&amp; = \alpha \int _{0}^{\infty} x^{\alpha - 1} e^{-x} dx \\
&amp; = \alpha \Gamma(\alpha)
\end{align*} \tag{5}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\Gamma(\alpha + 1) = \alpha \Gamma(\alpha) \tag{6}\]</span></p>
<p>易知 <span class="math inline">\(\Gamma(\alpha)\)</span> 函数是阶乘在实数集上的扩展，具有如下性质：</p>
<p><span class="math display">\[\Gamma(n) = (n-1)! \tag{7}\]</span></p>
<p>由 <span class="math inline">\((1)\)</span> <span class="math inline">\((3)\)</span> <span class="math inline">\((6)\)</span> 很容易求得 <span class="math inline">\(\gt 0\)</span> 自然数上的Gamma函数值，由 <span class="math inline">\((2)\)</span> <span class="math inline">\((4)\)</span> <span class="math inline">\((6)\)</span> 易求得小数的Gamma函数值</p>
<h4 id="gamma-分布">Gamma 分布</h4>
<p>对于 Gamma 函数有</p>
<p><span class="math display">\[\int_{0}^{+\infty} \frac{x^{\alpha-1} e^{-x}}{\Gamma{\alpha}} dx = 1 \tag{8}\]</span></p>
<p>因此可得 Gamma 分布的密度函数为</p>
<p><span class="math display">\[Gamma(x|\alpha) = \frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)} \tag{9}\]</span></p>
<p>更一般的令 <span class="math inline">\(x = \beta t\)</span> 可得</p>
<p><span class="math display">\[Gamma(t|\alpha, \beta) = \frac{\beta^{\alpha}t^{\alpha-1}e^{-\beta t}}{\Gamma(\alpha)} \tag{10}\]</span></p>
<p><span class="math inline">\(\alpha\)</span> 决定了 Gamma 分布的曲线形状， <span class="math inline">\(\beta\)</span> 决定了 Gamma 分布曲线的陡峭程度</p>
<p><img src="/images/posts/lda_topic/gamma-distribution.png"></p>
<h4 id="拓gamma-分布与-poisson-分布">【拓】Gamma 分布与 Poisson 分布</h4>
<p>参数为 <span class="math inline">\(\lambda\)</span> 的泊松分布概率为：</p>
<p><span class="math display">\[Poisson(X=k|\lambda) = \frac{\lambda ^{k} e-{\lambda}}{k!} \tag{11}\]</span></p>
<p>在 Gamma 分布的密度函数中取 <span class="math inline">\(\alpha = k+1\)</span> 可得</p>
<p><span class="math display">\[Gamma(x|\alpha = k+1) = \frac{x^{k}e{-x}}{\Gamma(k+1)} = \frac{x^{k}e^{-x}}{k!} \tag{12}\]</span></p>
<p>可见 <span class="math inline">\((11)\)</span> 和 <span class="math inline">\((12)\)</span> 在分布表达式上是一致的，区别在于 Poisson 分布是离散的，而 Gamma 分布是连续的</p>
<h3 id="共轭先验分布">共轭先验分布</h3>
<p>对于贝叶斯定理有</p>
<p><span class="math display">\[p(\theta | x) = \frac{p(x|\theta ) p(\theta)}{p(x)} \propto  p(x|\theta)p(\theta) \tag{13}\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是参数， <span class="math inline">\(p(\theta | x)\)</span> 是后验概率，<span class="math inline">\(p(\theta)\)</span> 是先验概率，<span class="math inline">\(p(x|\theta)\)</span> 称为给定参数 <span class="math inline">\(\theta\)</span> 下样本的似然概率</p>
<p>若后验概率 <span class="math inline">\(p(\theta|x)\)</span> 和先验概率 <span class="math inline">\(p(\theta)\)</span> 满足同样的分布律，那么先验分布和后验分布叫做共轭分布，同时先验分布叫作似然函数的共轭先验分布</p>
<h3 id="二项分布和-beta-分布">二项分布和 Beta 分布</h3>
<p>对于二项分布有</p>
<p><span class="math display">\[Binomial(k|n, p) = \binom{n}{k} p^{k}(1-p)^{n-k} \tag{14}\]</span></p>
<p>Beta 分布的概率密度函数为：</p>
<p><span class="math display">\[f(x) = \begin{cases}
\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} &amp; \text{ , } x\in [0, 1] \\ 
0 &amp; \text{ , } others
\end{cases} \tag{15}\]</span></p>
<p>其中系数 B 为</p>
<p><span class="math display">\[B(\alpha, \beta) = \int_{0}^{1} x^{\alpha -1}(1-x)^{\beta -1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} \tag{16}\]</span></p>
<p>因此对于 Beta 分布有</p>
<p><span class="math display">\[Beta(p|\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha -1}(1-p)^{\beta - 1} \tag{17}\]</span></p>
<p>观察 <span class="math inline">\((14)\)</span> 和 <span class="math inline">\((17)\)</span> 可知两者的密度函数非常相似，尝试将两个分布联合有</p>
<p><span class="math display">\[\begin{align*}
P(p|n, k, \alpha, \beta) &amp; \propto P(k|n, p) P(p|\alpha, \beta) \\
&amp; = Binomial(k|n, p) Beta(p|\alpha, \beta) \\
&amp; = \binom{n}{k}p^{k} (1-p)^{n-k} \times \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha - 1}(1-p)^{\beta - 1} \\
&amp; \propto p^{k+\alpha - 1}(1-p)^{n-k+\beta-1}
\end{align*} \tag{18}\]</span></p>
<p>归一化后可得后验概率</p>
<p><span class="math display">\[P(p|n, k, \alpha, \beta) = \frac{\Gamma (\alpha + \beta + n)}{\Gamma(\alpha + k) \Gamma (\beta + n - k)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1} \tag{19}\]</span></p>
<p>可以发现<strong>二项分布的共轭分布就是 Beta 分布</strong>，而且有</p>
<p><span class="math display">\[Beta(p|\alpha, \beta) + BinomialCount(k, n-k) = Beta(p|\alpha + k, \beta+n-k) \tag{20}\]</span></p>
<p><strong>注意！</strong> <span class="math inline">\((20)\)</span> 的 <span class="math inline">\(+\)</span> 并不是加法运算，可把它理解为<code>结合</code>，<span class="math inline">\(BinomialCount\)</span> 有数据得到的，也就是说 <span class="math inline">\((20)\)</span> 式满足 <code>先验 + 数据 = 后验</code>，称为 <code>Beta-Binomial 共轭</code></p>
<h4 id="beta分布的期望">Beta分布的期望</h4>
<p><span class="math display">\[\begin{align*}
E(Beta(p|\alpha, \beta)) &amp; = \int_{0}^{1} t Beta(p|\alpha, \beta) dt \\
&amp; = \int_{0}^{1} t \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} t^{\alpha - 1} (1-t)^{\beta - 1} dt \\
&amp; = \int_{0}^{1} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} t^{\alpha} (1-t)^{\beta - 1} dt
\end{align*} \tag{21}\]</span></p>
<p>又有</p>
<p><span class="math display">\[\int_{0}^{1} \frac{\Gamma(\alpha + \beta + 1)}{\Gamma(\alpha + 1)\Gamma(\beta)} p^{\alpha} (1-p)^{\beta-1} = 1 \tag{22}\]</span></p>
<p>由 <span class="math inline">\((21) (22)\)</span> 可得</p>
<p><span class="math display">\[E(Beta(p|\alpha, \beta)) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\ \frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} = \frac{\alpha}{\alpha+\beta} \tag{23}\]</span></p>
<h3 id="多项分布和-dirichlet-分布">多项分布和 Dirichlet 分布</h3>
<p>多项分布是二项分布在多种( <span class="math inline">\(&gt;2\)</span> )状态下的推广，对于多项分布有：</p>
<p><span class="math display">\[Multinomial(\underset{m}{\rightarrow}|n, \underset{p}{\rightarrow}) = \binom{n}{\underset{m}{\rightarrow}}\prod_{k=1}^{K} p_{k}^{m_{k}} \tag{24}\]</span></p>
<p>Dirichlet 分布是 Beta 分布在多维状态下的推广，Dirichlet 分布的密度函数为</p>
<p><span class="math display">\[f(\underset{p}{\rightarrow}|\underset{\alpha}{\rightarrow}) = \begin{cases}
\frac{1}{\Delta (\underset{\alpha}{\rightarrow})} \prod_{k=1}^{K} p_{k}^{\alpha_{k-1}} &amp; \text{ , } p_{k} \in [0,1] \\ 
0 &amp; \text{ , } others 
\end{cases} \tag{25}\]</span></p>
<p>可知其密度函数的自由度为 <span class="math inline">\(k-1\)</span></p>
<p>对于系数有</p>
<p><span class="math display">\[\Delta(\underset{\alpha}{\rightarrow}) = \frac{\prod_{k=1}^{K} \Gamma(\alpha_{k})}{\Gamma(\sum_{k=1}^{K} \alpha_{k})} \tag{26}\]</span></p>
<p>因此 Dirichlet 分布可表示为</p>
<p><span class="math display">\[Dir(\underset{p}{\rightarrow}|\underset{\alpha}{\rightarrow}) = \frac{\Gamma(\sum_{k=1}^{K}\alpha _{k})}{\prod_{k=1}^{K}\Gamma(\alpha_{k})} \prod_{k-1}^{K} p_{k}^{\alpha _{k-1}} \tag{27}\]</span></p>
<p>Dirichlet 分布的期望和 Beta 分布差不多</p>
<p><span class="math display">\[E(p_{i}) = \frac{\alpha_{i}}{\sum_{k=1}^{K} \alpha_{k}} \tag{28}\]</span></p>
<p>可知<strong>多项分布是二项分布的推广</strong>，<strong>Dirichlet 分布是 Beta 分布的推广</strong></p>
<p>多项分布和 Dirichlet 分布也同样满足共轭关系，因此有</p>
<p><span class="math display">\[Dir(\underset{p}{\rightarrow} | \underset{\alpha}{\rightarrow}) + MultinomialCount(\underset{m}{\rightarrow}) = Dir(\underset{p}{\rightarrow} | \underset{\alpha}{\rightarrow} + \underset{m}{\rightarrow}) \tag{29}\]</span></p>
<p>这里的 <span class="math inline">\(+\)</span> 也是代表<code>结合</code>的意思，并不是加法，因此同样满足 <code>先验 + 数据 = 后验</code>，称为 <code>Dirichlet-Multinomial 共轭</code></p>
<h4 id="对称-dirichlet-分布">对称 Dirichlet 分布</h4>
<p>在具体实现中为了简化计算，对于参数 <span class="math inline">\(\underset{\alpha}{\rightarrow} = (\alpha _{1}, \alpha _{2}, ..., \alpha _{k})\)</span> 令 <span class="math inline">\(\alpha _{1} = \alpha _{2} = ... = \alpha _{k} = \alpha\)</span> , 满足这样的参数称为对称 Dirchlet 分布，对于 <span class="math inline">\(\alpha\)</span> 的取值有</p>
<ul>
<li><span class="math inline">\(\alpha = 1\)</span> : 退化为均匀分布</li>
<li><span class="math inline">\(\alpha \gt 1\)</span> : <span class="math inline">\(p_{1} = p_{2} = ... = p_{k}\)</span> 的概率增大</li>
<li><span class="math inline">\(\alpha \lt 1\)</span> : <span class="math inline">\(p_{i}=1\)</span>, <span class="math inline">\(p_{\neq i} = 0\)</span> 的概率增大</li>
</ul>
<img src="/images/posts/lda_topic/dirichlet-alpha.gif">
<p align="center">
图片来自维基百科
</p>
<h3 id="硬-软思维">硬 &amp; 软思维</h3>
<p>举个例子，判断一个人的性别</p>
<ul>
<li>硬分类思想：这个人是男的（很果断，非0即1）</li>
<li>软分类思想：这个人 70% 的可能是男的， 30% 的可能是女的，所以他应该是个男的</li>
</ul>
<h2 id="再话-lda">再话 LDA</h2>
<p>在 <code>LDA 的解释</code> 部分已经解释了 LDA 的基本思想，这里结合背景知识再深入的讲解</p>
<p>假设有 M 篇文档，对应第 d 个文档种有 <span class="math inline">\(N_{d}\)</span> 词</p>
<p><img src="/images/posts/lda_topic/corpus.png"></p>
<p>我们的目标是找到每一篇文档的主题分布和每一个主题中词的分布。</p>
<p>我们假设<code>隐变量K</code>代表主题数，则 LDA 的概率图模型为</p>
<p><img src="/images/posts/lda_topic/bayes-model.png"></p>
<p>图中只有 <code>Observed Word</code> 是可见的，其他都是<code>隐</code>的，<code>Observed word</code> <span class="math inline">\(W _{d, n}\)</span> 代表第 d 个文档的第 n 个词</p>
<p>LDA 假设文档的主题的先验分布是 Dirichlet 分布，即对于任一文档 d ，其主题分布 <span class="math inline">\(\theta _{d}\)</span> 为</p>
<p><span class="math display">\[\theta _{d} = Dir(\underset{\alpha}{\rightarrow})\]</span></p>
<p>其中 <span class="math inline">\(\alpha \in \mathbb{R}^{K}\)</span> 是分布的超参数，是一个 <span class="math inline">\(K\)</span> 维向量（它对应了文档分别属于 <span class="math inline">\(K\)</span> 个主题的软概率）</p>
<p>LDA 假设主题中词的先验分布是 Dirichlet 分布，即对于任一主题 <span class="math inline">\(k\)</span>，其词分布 <span class="math inline">\(\beta _{k}\)</span> 为</p>
<p><span class="math display">\[\beta_{k} = Dir(\underset{\eta }{\rightarrow})\]</span></p>
<p>其中 <span class="math inline">\(\eta \in \mathbb{R}^{V}\)</span> 为分布的超参数，<span class="math inline">\(V\)</span> 代表词典的大小， 是一个 <span class="math inline">\(V\)</span> 维的向量（它对应了词典中各个词对主题影响的软概率）</p>
<p>对于数据中任意一篇文档 <span class="math inline">\(d\)</span> 中的第 n 个词，可以从主题分布 <span class="math inline">\(\theta_{d}\)</span> 中得到它的主题编号 <span class="math inline">\(z_{d,n}\)</span> 的分布为</p>
<p><span class="math display">\[z_{d,n} = Multinomial(\theta_{d})\]</span></p>
<p>而对于该主题编号，可以得到我们看到词 <span class="math inline">\(w_{d, n}\)</span> 的概率分布为</p>
<p><span class="math display">\[w_{d, n} = Multinomial(\beta_{z_{d, n}})\]</span></p>
<p>对于 M 篇文档就有 M 个文档主题的 Dirichlet 分布，而对应的<strong>数据</strong>就有 M 个主题编号的多项分布，因此 <span class="math inline">\(\alpha \rightarrow \theta_{d} \rightarrow \underset{z_{d}}{\rightarrow}\)</span> 满足 Dirichlet-Multinomial 共轭</p>
<p>那么数据似然部分，如何求解呢？</p>
<p>假设在第 k 个主题中，第 v 个词的个数为 <span class="math inline">\(n_{k}^{(v)}\)</span>，则该主题编号对应的词多项分布计数可表示为</p>
<p><span class="math display">\[\underset{n_{k}}{\rightarrow} = (n_{k}^{(1)}, n_{k}^{(2)}, ..., n_{k}^{(V)})\]</span></p>
<p>利用 Dirichlet-Multinomial 共轭，可得 <span class="math inline">\(\beta_{k}\)</span> 的后验分布为</p>
<p><span class="math display">\[Dir(\beta_{k}|\underset{\eta}{\rightarrow}+\underset{n_{k}}{\rightarrow})\]</span></p>
<p><strong>由于主题产生词不依赖具体的某一个文档，因此文档主题分布和词分布是独立的</strong></p>
<p>可能上面的解释还是难以理解，下面尝试以更通俗的语言解释</p>
<blockquote>
<p>这里的超参数一般是我们根据经验设定一个具体的数，这样也就是得到了我们的最原始的先验分布 初始化：利用先验分布，为每一个文档 m 中的每一个词 n 赋予一个主题 z 根据这个初始化 我们可以统计文档m的各个主题数目，也可以统计某个主题z对应的不同词的数目 这些数目我们看做第一步的数据似然 -&gt; 然后再结合先验 -&gt; 得到第一步的后验 把第一步的后验看做第二步的先验概率再去为每一个文档 m 中的每一个词 n 赋予一个主题 z，因为是共轭先验分布因此进行和第一步一样的迭代，直到收敛 —— 上述解释来自 <a href="https://www.cnblogs.com/pinard/p/6831308.html" target="_blank" rel="noopener">kylin0228</a></p>
</blockquote>
<p>本文只介绍原理部分，实现部分后期发布</p>
<h2 id="refrence">Refrence</h2>
<ul>
<li>[1] https://en.wikipedia.org/wiki/Dirichlet_distribution</li>
<li>[2] http://www.flickering.cn/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/2014/06/lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1/</li>
<li>[3] https://www.cnblogs.com/pinard/p/6831308.html</li>
<li>[4] http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/10/20/tensorflow常用函数以及技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/tensorflow常用函数以及技巧/" itemprop="url">tensorflow常用函数以及技巧</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-20T14:52:21+08:00">
                2018-10-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/20/tensorflow常用函数以及技巧/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/20/tensorflow常用函数以及技巧/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我常用的深度学习框架有两种 pytorch 和 tensorflow ,在学校的时候习惯用 pytorch ，实习工作时主要用 tensorflow. 本文主要记录我常用的 tensorflow 函数以及一些使用实例</p>
<h2 id="常用函数">常用函数</h2>
<h3 id="casting">Casting</h3>
<p>主要用于类型的转换，常用的函数有:</p>
<ul>
<li>tf.cast(x, dtype, name=None)</li>
</ul>
<h3 id="reduce">Reduce</h3>
<ul>
<li>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None) 在指定维度上求和</li>
<li>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) 在指定维度上求最大值</li>
<li>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) 在指定维度上求最小值</li>
<li>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) 在指定维度上求均值</li>
</ul>
<h3 id="实例一求张量的-mask-和长度">实例一：求张量的 mask 和长度</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># placeholder</span></span><br><span class="line">inputs = tf.placeholder(tf.int32,</span><br><span class="line">                        shape=[<span class="keyword">None</span>, <span class="keyword">None</span>],</span><br><span class="line">                        name=<span class="string">'inputs'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取inputs的mask，</span></span><br><span class="line">inputs_mask = tf.cast(inputs, tf.bool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 inputs 在0轴的长度</span></span><br><span class="line">inputs_len = tf.reduce_sum(tf.cast(inputs_mask, tf.int32), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">mask, lens = sess.run([inputs_mask, inputs_len],</span><br><span class="line">                     feed_dict=&#123;inputs: x&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Mask:\n"</span>, mask)</span><br><span class="line">print(<span class="string">"Length:\n"</span>, lens)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Mask:</span><br><span class="line"> [[ True  True  True]</span><br><span class="line"> [ True  True False]</span><br><span class="line"> [ True False False]]</span><br><span class="line">Length:</span><br><span class="line"> [3 2 1]</span><br></pre></td></tr></table></figure>
<h3 id="shape-与-shape-变换">shape 与 shape 变换</h3>
<ul>
<li>tf.shape(input, name=None) 获取张量的形状</li>
<li>tf.reshape(tensor, shape, name=None) 变换张量形状</li>
<li>tf.squeeze(input, squeeze_dims=None, name=None) 压缩(去除)张量维度（压缩形状大小为1的维度）</li>
<li>tf.expand_dims(input, dim, name=None) 扩充张量维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">inputs = tf.placeholder(tf.int32, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">"inputs"</span>)</span><br><span class="line">print(sess.run(tf.shape(inputs)))  <span class="comment"># (1, 2, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 挤压维度，因为第0维的大小为1故可以去除</span></span><br><span class="line">inputs1 = tf.squeeze(inputs, <span class="number">0</span>)</span><br><span class="line">print(sess.run(tf.shape(inputs1))) <span class="comment"># (2, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩充维度</span></span><br><span class="line">inputs2 = tf.expand_dims(inputs1, <span class="number">0</span>)</span><br><span class="line">print(sess.run(tf.shape(inputs2))) <span class="comment"># (1, 2, 3)</span></span><br></pre></td></tr></table></figure>
<h3 id="slicing-and-joining-切片和连接">Slicing and Joining 切片和连接</h3>
<h4 id="tf.sliceinput_-begin-size-namenone">tf.slice(input_, begin, size, name=None)</h4>
<p>tf的切片略难理解，先理解函数参数的意义</p>
<ul>
<li>input_：张量</li>
<li>begin: n维列表，begin[i] 表示从inputs中第i轴抽取数据时，从第i轴的begin[i]个元素开始抽取数据</li>
<li>size: n维列表，size[i]表示要抽取的第i轴元素的数目</li>
</ul>
<p>举个例子 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">inputs = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">                      [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">                      [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">slice_1: [[[3 3 3]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第0轴： begin[0] = 1 代表从第1个元素开始取，size[0] = 1 代表取一个元素， </span></span><br><span class="line"><span class="string">        所以第0轴的结果是 [[[3, 3, 3], [4, 4, 4]]]</span></span><br><span class="line"><span class="string">第1轴： begin[1] = 0 代表从第0个元素开始取，size[1] = 1 代表取一个元素,</span></span><br><span class="line"><span class="string">        故得到 [[[3, 3, 3]]]</span></span><br><span class="line"><span class="string">第2轴： begin[2] = 0 代表从第0个元素开始取，size[2] = 3 代表取两个元素</span></span><br><span class="line"><span class="string">        故得到 [[[3, 3]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">所以最后结果为[[[3, 3]]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"slice_1"</span>)</span><br><span class="line">print(sess.run(tf.slice(inputs, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>], name=<span class="string">"slice_1"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">slice_2: [[[3 3 3]</span></span><br><span class="line"><span class="string">           [4 4 4]]</span></span><br><span class="line"><span class="string">          [[5 5 5]</span></span><br><span class="line"><span class="string">           [6 6 6]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-1 代表取完该维度的所有元素</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"slice_2"</span>)</span><br><span class="line">print(sess.run(tf.slice(inputs, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>])))</span><br></pre></td></tr></table></figure></p>
<h4 id="tf.splitvalue-num_or_size_splits-axis0-numnone-namesplit">tf.split(value, num_or_size_splits, axis=0, num=None, name=‘split’)</h4>
<ul>
<li>value: 张量</li>
<li>num_or_size_splits：int / list / tensor 指定切割大小</li>
<li>axis：指定切割维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">inputs = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">在第0轴上切割</span></span><br><span class="line"><span class="string">[[1 2 3]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[4 5 6]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs = tf.split(inputs, <span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">在第1轴上切割</span></span><br><span class="line"><span class="string">[[1]</span></span><br><span class="line"><span class="string"> [4]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[2]</span></span><br><span class="line"><span class="string"> [5]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[3]</span></span><br><span class="line"><span class="string"> [6]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs1 = tf.split(inputs, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">指定每个切割区间大小(各区间大小之和应该等于该维度大小)</span></span><br><span class="line"><span class="string">[[1]</span></span><br><span class="line"><span class="string"> [4]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[2 3]</span></span><br><span class="line"><span class="string"> [5 6]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs2 = tf.split(inputs, [<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf.tileinput-multiples-namenone">tf.tile(input, multiples, name=None)</h4>
<ul>
<li>input：输入张量</li>
<li>multiples：list 指定每个维度重复多少次</li>
</ul>
<p>第i维的输出为 input.dims(i) * multiples[i], tf.tile 常用于 <code>trilinear</code> 等操作上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">inputs = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], </span><br><span class="line">                      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">在最后扩充一维，并重复三次</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[[1 1 1]</span></span><br><span class="line"><span class="string">  [2 2 2]</span></span><br><span class="line"><span class="string">  [3 3 3]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[4 4 4]</span></span><br><span class="line"><span class="string">  [5 5 5]</span></span><br><span class="line"><span class="string">  [6 6 6]]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">expands = tf.tile(tf.expand_dims(inputs, <span class="number">2</span>), [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h4 id="tf.padtensor-paddings-modeconstant-namenone-constant_values0">tf.pad(tensor, paddings, mode=‘CONSTANT’, name=None, constant_values=0)</h4>
<ul>
<li>paddings： 是一个张量，[N, 2]形式，N代表张量的阶，2代表必须是2列</li>
<li>mode：可以取三个值
<ul>
<li>CONSTANT：常量填充，值可由 constant_value 设定</li>
<li>REFLECT： 映射填充，上下（axis=0）填充顺序和paddings是相反的，左右（axis=1）顺序补齐</li>
<li>SYMMETRIC：对称填充，上下（axis=0）填充顺序是和paddings相同的，左右（axis=1）对称补齐</li>
</ul></li>
</ul>
<p>填充的方向一般是有<code>向前和向后</code>填充 ， pad函数也是类似的，paddings 必须是两列用于指定维度的前后填充大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">inputs = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>]], </span><br><span class="line">                      [[<span class="number">2</span>, <span class="number">2</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">第0轴：[1, 1] 指定向前和向后填充1个单位</span></span><br><span class="line"><span class="string">第1轴：[1, 1] 指定向前和向后填充1个单位</span></span><br><span class="line"><span class="string">第2轴：[1, 2] 指定向前填充1个单位向后填充2个单位</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[[0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 1 1 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 2 2 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]</span></span><br><span class="line"><span class="string">  [0 0 0 0 0]]] </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs = tf.pad(inputs, [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="tf.concatvalues-axis-nameconcat">tf.concat(values, axis, name=‘concat’)</h4>
<p>在指定维度拼接.</p>
<h4 id="tf.stackvalues-axis0-namepack">tf.stack(values, axis=0, name=”pack”)</h4>
<p>将一个R维张量列表沿着axis轴组合成一个R+1维的张量。作用有点像 python 的 zip 函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">y = [<span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[1 3]</span></span><br><span class="line"><span class="string"> [2 4]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs = tf.stack([x, y], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[1 2]</span></span><br><span class="line"><span class="string"> [3 4]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">outputs = tf.stack([x, y], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf.unstackvalue-numnone-axis0-nameunstack">tf.unstack(value, num=None, axis=0, name=‘unstack’)</h4>
<p>tf.stack 的反过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">     [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[1 3]</span></span><br><span class="line"><span class="string">[2 4]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">a, b = tf.unstack(x, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf.reverse_sequenceinput-seq_lengths-seq_axisnone-batch_axisnone-namenone-seq_dimnone-batch_dimnone">tf.reverse_sequence(input, seq_lengths, seq_axis=None, batch_axis=None, name=None, seq_dim=None, batch_dim=None)</h4>
<p>常用在双向的网络上，如双向的RNN</p>
<h4 id="tf.transposea-permnone-nametranspose-conjugatefalse">tf.transpose(a, perm=None, name=‘transpose’, conjugate=False)</h4>
<p>维度位置交换函数，此函数用的比较频繁</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                 [<span class="number">3</span>, <span class="number">4</span>]]])</span><br><span class="line"></span><br><span class="line">y = tf.transpose(x, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">x shape: [1 2 2]</span></span><br><span class="line"><span class="string">x: [[[1 2]</span></span><br><span class="line"><span class="string">  [3 4]]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"x shape:"</span>, sess.run(tf.shape(x)))</span><br><span class="line">print(<span class="string">"x:"</span>, sess.run(x))</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">y shape: [2 1 2]</span></span><br><span class="line"><span class="string">y: [[[1 2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[3 4]]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"y shape:"</span>, sess.run(tf.shape(y)))</span><br><span class="line">print(<span class="string">"y:"</span>, sess.run(y))</span><br></pre></td></tr></table></figure>
<h4 id="tf.gatherparams-indices-validate_indicesnone-namenone-axis0">tf.gather(params, indices, validate_indices=None, name=None, axis=0)</h4>
<p>取得 axis 轴指定下标 indices 的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], </span><br><span class="line">                 [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[2 4]</span></span><br><span class="line"><span class="string"> [6 8]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">y = tf.gather(x, [<span class="number">1</span>, <span class="number">3</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="sequence">Sequence</h3>
<h4 id="tf.sequence_masklengths-maxlennone-dtypetf.bool-namenone">tf.sequence_mask(lengths, maxlen=None, dtype=tf.bool, name=None)</h4>
<p>构建序列<code>长度</code>的mask标志，是以长度来构建的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">lens = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) </span><br><span class="line">max_len = tf.reduce_max(lens)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ True False False]</span></span><br><span class="line"><span class="string"> [ True  True False]</span></span><br><span class="line"><span class="string"> [ True  True  True]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">mask = tf.sequence_mask(lens, max_len)</span><br></pre></td></tr></table></figure>
<h2 id="常用技巧">常用技巧</h2>
<h3 id="attention的计算">Attention的计算</h3>
<p>attention其实并不是什么高大上的东西，从数学角度上看其实原理很简单，如果对Attention机制能有较深的理解且能实现，那么对于大部分的 paper 都可以实现了。</p>
<h4 id="attention-机制的简单理解">Attention 机制的简单理解</h4>
<p>假设有三个值 <span class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span> (<span class="math inline">\(Q\)</span>代表 <span class="math inline">\(Query\)</span>, <span class="math inline">\(K\)</span>代表<span class="math inline">\(Key\)</span>，<span class="math inline">\(V\)</span>代表<span class="math inline">\(Value\)</span>)，其中 <span class="math inline">\(K\)</span> 跟 <span class="math inline">\(V\)</span> 是匹配的，那么Attention可以认为是通过 <span class="math inline">\(Query\)</span> 去注意 <span class="math inline">\(Value\)</span> 中重要的信息</p>
<p><span class="math display">\[\begin{align*}
&amp; sim = Similarity(Q, K) \\
&amp; \alpha  = softmax(sim) \\ 
&amp; V_{attn} = \alpha \cdot V
\end{align*}\]</span></p>
<p>其中 Simlarity 是计算相似度的函数，最简单的可通过MLP来计算，在NLP中一般有 <span class="math inline">\(K = V\)</span></p>
<h4 id="一个简单的例子">一个简单的例子</h4>
<p>实现一个简单的 self atttion，self attention中 <span class="math inline">\(Q = K = V\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(value)</span>:</span></span><br><span class="line">    Q = value</span><br><span class="line">    K = value</span><br><span class="line">    V = value</span><br><span class="line"></span><br><span class="line">    shape = V.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    W = tf.get_variable(<span class="string">'attn_W'</span>, </span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], shape[<span class="number">-1</span>]],</span><br><span class="line">                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">    U = tf.get_variable(<span class="string">'attn_U'</span>,</span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], shape[<span class="number">-1</span>]],</span><br><span class="line">                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">    P = tf.get_variable(<span class="string">'attn_P'</span>,</span><br><span class="line">                        shape=[shape[<span class="number">-1</span>], <span class="number">1</span>], </span><br><span class="line">                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">    Q = tf.reshape(Q, [<span class="number">-1</span>, shape[<span class="number">-1</span>]])</span><br><span class="line">    K = tf.reshape(K, [<span class="number">-1</span>, shape[<span class="number">-1</span>]])</span><br><span class="line">    </span><br><span class="line">    sim = tf.tanh(tf.add(</span><br><span class="line">        tf.matmul(Q, W), </span><br><span class="line">        tf.matmul(K, U)))</span><br><span class="line">    alpha = tf.nn.softmax(</span><br><span class="line">        tf.reshape(tf.matmul(sim, P), [<span class="number">-1</span>, shape[<span class="number">1</span>], <span class="number">1</span>]), axis=<span class="number">1</span>)</span><br><span class="line">    V_attn = tf.multiply(alpha, V)</span><br><span class="line">    <span class="keyword">return</span> V_attn, alpha</span><br><span class="line"></span><br><span class="line">inputs = tf.random_normal([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line">attn, alpha = attention(inputs)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"inputs:\n"</span>, sess.run(inputs))</span><br><span class="line">print(<span class="string">"alpha:\n"</span>, sess.run(alpha))</span><br><span class="line">print(<span class="string">"attn applied:\n"</span>, sess.run(attn))</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/10/07/常用的相似度计算方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/07/常用的相似度计算方法/" itemprop="url">常用的相似度计算方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-07T23:50:57+08:00">
                2018-10-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/07/常用的相似度计算方法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/07/常用的相似度计算方法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>不同样本间相似度计算也可以认为是样本间距离的计算，本文主要总结几种常见的距离计算方法</p>
<h2 id="闵可夫斯基距离minkowski">闵可夫斯基距离(Minkowski)</h2>
<p><span class="math display">\[\begin{align*}
distance(X, Y) = (\sum_{i=1}^{m}|x_{i} - y_{i}|^{p})^{\frac{1}{p}} \\\\
where\ X = (x_{1}, ..., x_{m}), Y = (y_{1}, ..., y_{m})
\tag{1}
\end{align*}\]</span></p>
<p>特别的当</p>
<ul>
<li><span class="math inline">\(p = 1\)</span> 时称为<code>曼哈顿距离</code></li>
<li><span class="math inline">\(p = 2\)</span> 时称为 <code>欧式距离</code></li>
<li><span class="math inline">\(p \rightarrow \infty\)</span> 时称为<code>切比雪夫距离</code></li>
</ul>
<h3 id="曼哈顿距离">曼哈顿距离</h3>
<p><span class="math display">\[distance(X, Y) = \sum_{i=1}^{m}|x_{i} - y_{i}| \tag{2}\]</span></p>
<p>曼哈顿距离又称<code>街区距离</code>，从起点到终点只能往东南西北四个方向走，不能斜着走</p>
<h3 id="欧式距离">欧式距离</h3>
<p><span class="math display">\[distance(X, Y) = (\sum_{i=1}^{m}|x_{i} - y_{i}|^{2})^{\frac{1}{2}} \tag{3}\]</span></p>
<h3 id="曼哈顿距离-vs-欧式距离">曼哈顿距离 vs 欧式距离</h3>
<p><img src="/images/posts/distances/01.png"></p>
<p>其中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离</p>
<h2 id="余弦相似度-cosine-similarity">余弦相似度 (cosine similarity)</h2>
<p><span class="math display">\[\begin{align*}
cos(\theta) &amp;= \frac{\underset{a}{\rightarrow} \cdot \underset{b}{\rightarrow}}{\left \| \underset{a}{\rightarrow} \right \| \left \| \underset{b}{\rightarrow} \right \|} \\
&amp;=  \frac{\sum_{i=1}^{m} x_{i} y_{i}}{\sqrt{\sum_{i=1}^{m} x_{i}^{2}} \sqrt{\sum_{i=1}^{m} y_{i}^{2}}} \\
&amp; where \underset{a}{\rightarrow} = (x_{1}, x_{1}, ..., x_{n}),\ \underset{b}{\rightarrow} = (y_{2}, y_{2}, ..., y_{n})
\end{align*} \tag{4}\]</span></p>
<p>由于文档常用VSM形式表示，可以把文档当作高维向量，因此文档之间的相似度一般可以用余弦相似度计算</p>
<h3 id="欧式距离-vs-余弦相似度">欧式距离 vs 余弦相似度</h3>
<p><img src="/images/posts/distances/02.png"></p>
<h2 id="皮尔逊相关系数">皮尔逊相关系数</h2>
<p><span class="math display">\[\begin{align*}
\rho_{XY} &amp; = \frac{cov(X, Y)}{\sigma_{X} \sigma_{Y}} \\
&amp; = \frac{E[(X-E_{x})(Y-E_{Y})]}{\sigma_{X} \sigma_{Y}} \\
&amp; = \frac{\sum_{i=1}^{m} (X_{i} - \mu_{X}) (Y_{i} - \mu_{Y})}{\sqrt{\sum_{i=1}^{m} (X_{i} - \mu_{X})^{2}} \sqrt{\sum_{i=1}^{m} (Y_{i} - \mu_{Y})^{2}}}
\end{align*} \tag{5}\]</span></p>
<p>其中 <span class="math inline">\(cov(X, Y)\)</span> 为 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 的协方差, <span class="math inline">\(\sigma_{X}\)</span> 为 <span class="math inline">\(X\)</span> 的标准差, <span class="math inline">\(\sigma_{Y}\)</span> 为 <span class="math inline">\(Y\)</span> 的标准差</p>
<p>皮尔逊相关系数是衡量随机变量 <span class="math inline">\(X\)</span> 与 $Y $ 相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）</p>
<h3 id="皮尔逊相关系数-vs-余弦相似度">皮尔逊相关系数 vs 余弦相似度</h3>
<p>对比一下 <span class="math inline">\((4)\)</span> 式和 <span class="math inline">\((5)\)</span> 可以发现，其实相关系数和余弦相似度的关系是平移关系，也可以认为余弦相似度是相关系数<code>去均值化</code>的结果</p>
<h2 id="杰卡德相似系数-jaccard">杰卡德相似系数 (Jaccard)</h2>
<p><span class="math display">\[J(A, B) = \frac{|A\cap B|}{|A \cup B|} \tag{6}\]</span></p>
<p>两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数, 杰卡德系数一般用于度量两个集合的相似度</p>
<h2 id="相对熵-kl距离">相对熵 / KL距离</h2>
<p><span class="math display">\[D_{KL}(p||q) = \sum_{x} p(x) log \frac{p(x)}{q(x)} \tag{7}\]</span></p>
<p>计算的是两个分布之间的距离，特别要注意<code>KL距离</code>是非对称的，也就是说 <span class="math inline">\(p\)</span> 跟 <span class="math inline">\(q\)</span> 互换位置得到的结果是不一致的，一般来说当 <span class="math inline">\(p\)</span> <span class="math inline">\(q\)</span> 是同分布时才能得到一样的结果</p>
<h2 id="refrence">Refrence</h2>
<p>[1] http://www.ehcoo.com/ManhattanDistance.html</p>
<p>[2] https://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/10/03/使用numpy实现一个深度学习框架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/03/使用numpy实现一个深度学习框架/" itemprop="url">使用numpy实现一个深度学习框架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-03T11:35:16+08:00">
                2018-10-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/03/使用numpy实现一个深度学习框架/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/03/使用numpy实现一个深度学习框架/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>为了理解深度学习框架的大致机理，决定使用numpy实现一个简单的神经网络框架</p>
<p>深度学习框架我觉得最重要的是实现了<code>链式求导法则</code>，而计算图就是建立在<code>链式求导法则</code>之上的，目前大多数深度学习是基于<code>反向传播</code>思想的，如何在链式计算图中进行<code>前向传播</code>和<code>反向传播</code>是深度学习框架重点要考虑的</p>
<p><img src="/images/posts/simnet/01.png"></p>
<p align="center">
计算图
</p>
<p>目前主流的深度学习框架有两种计算图实现方式：</p>
<ul>
<li>静态图：先构建好图再让数据流入，优点是结构清晰，理论上速度占优（实际不一定）；缺点是debug困难，对初学者学习不友好（我认为主要是对计算图的理解不深刻导致的）。典型的框架代表：Tensorflow</li>
<li>动态图：程序按照编写的顺序动态执行，优点是所见所得，可以随时输出动态结果，调试方便，典型的框架代表：PyTorch</li>
</ul>
<p>个人现在更倾向于使用<code>tensorflow</code>，一方面是<code>tensorflow</code>生态较成熟，另一方面个人感觉静态图更<code>数学化</code>，而动态图更<code>工程化</code></p>
<p>不过本文实现的是一个类似pytorch风格的动态图框架</p>
<h2 id="知识储备">知识储备</h2>
<h3 id="链式求导法则">链式求导法则</h3>
<p><span class="math display">\[\begin{align*}
&amp; f{&#39;} = f(x) \\\\
&amp; g{&#39;} = g(f{&#39;}) \\\\
&amp; y{&#39;} = k(g{&#39;}) \\\\
&amp; loss = L(y, y{&#39;}) 
\end{align*}\]</span></p>
<p>用链式求导法则计算可得</p>
<p><span class="math display">\[\begin{align*}
\frac{d(loss)}{d(x)} = \frac{d(f{&#39;})}{d(x)} \times \frac{d(g{&#39;})}{d(f{&#39;})} \times \frac{d(y{&#39;})}{d(g{&#39;})} \times \frac{d(loss)}{y{&#39;}}\\\\
\tag{1}
\end{align*}
\]</span></p>
<h3 id="常见的激活函数">常见的激活函数</h3>
<p>激活函数几乎都是非线性且尽可能全局可导的，激活函数的选择是很重要的，<code>sigmoid</code>, <code>tanh</code> 由于自身函数的特点，只有很小的区间梯度变化比较明显，大部分的区间梯度变化很小很容易造成梯度消失，本人更倾向于使用<code>relu</code>，或者是 <code>batch normalization</code>和<code>tanh</code>搭配使用</p>
<h4 id="sigmoid">sigmoid</h4>
<p><span class="math display">\[\sigma (z) = \frac{1}{1+e^{-z}} \tag{2}\]</span></p>
<p>用matplotlib画出它的图像 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">0.2</span>)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">"sigmoid"</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/posts/simnet/sigmoid.png"></p>
<h4 id="tanh">tanh</h4>
<p><span class="math display">\[tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \tag{3}\]</span></p>
<p>用matplotlib画出它的图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">0.2</span>)</span><br><span class="line">y = tanh(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">"tanh"</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/simnet/tanh.png"></p>
<h4 id="relu">relu</h4>
<p><span class="math display">\[relu(z) = max(0, z) = relu(z) = \begin{cases}
0 &amp; \text{ if } x \leqslant 0 \\ 
z &amp; \text{ if } x &gt; 0 
\end{cases} \tag{4}\]</span></p>
<p>用matplotlib画出它的图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">0.2</span>)</span><br><span class="line">y = relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">"relu"</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/simnet/relu.png"></p>
<h3 id="常见的优化器">常见的优化器</h3>
<p>SGD、Adam等，推荐阅读本人的另一篇文章 <a href="https://seanlee97.github.io/2018/10/01/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">常用的梯度下降优化算法</a></p>
<h3 id="常见的损失函数">常见的损失函数</h3>
<h4 id="mse">MSE</h4>
<p><span class="math display">\[L(x_{i}, y_{i}) = (x_{i} - y_{i})^{2} \tag{5}\]</span></p>
<h4 id="nll-负对数似然">NLL 负对数似然</h4>
<p><span class="math display">\[L(x, label) = -x_{label} \tag{6}\]</span></p>
<h4 id="bce">BCE</h4>
<p>BinCrossEntropy 是二分类用的交叉熵</p>
<p><span class="math display">\[L(x_{i}, y_{i}) = -w_{i}[y_{i} logx_{i} + (1-y_{i})log(1-x_{i})]  \tag{7}\]</span></p>
<h4 id="crossentropy-交叉熵">CrossEntropy 交叉熵</h4>
<p><span class="math display">\[\begin{align*}
L(x, label) &amp; = -w_{label} log\frac{e^{x_label}}{\sum_{j=1}^{N} e^{x_{j}}} \\\\
&amp; = w_{label} [-x_{label} + log\sum_{j=1}^{N} e^{x_{j}}]
\end{align*} \tag{8}\]</span></p>
<h2 id="目录结构">目录结构</h2>
<ul>
<li>nn：核心的网络包
<ul>
<li>NN: 基础网络类</li>
<li>Linear：全连接层</li>
<li>Variable：基础参数类</li>
<li>ReLU / Sigmoid / Tanh：激活函数</li>
</ul></li>
<li>optim：优化器
<ul>
<li>SGD</li>
<li>Adam</li>
</ul></li>
<li>loss：损失函数
<ul>
<li>MSE</li>
<li>CrossEntropy</li>
</ul></li>
<li>init：初始化器
<ul>
<li>Normal：高斯分布</li>
<li>TruncatedNormal：截断高斯分布</li>
<li>Uniform：均匀分布</li>
</ul></li>
<li>fn.py：激活函数</li>
<li>pipe.py：pipeline</li>
</ul>
<h2 id="实现细节">实现细节</h2>
<h3 id="nn">NN</h3>
<p><code>NN</code> 是最基础的网络基类，所有定义的网络都要继承该类 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NN</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.forward(*args)</span><br></pre></td></tr></table></figure></p>
<h3 id="variable">Variable</h3>
<p>用于保存可导变量，求导时会用到此类封装的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Variable</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, wt, dw, b, db)</span>:</span></span><br><span class="line">        self.wt = wt</span><br><span class="line">        self.dw = dw</span><br><span class="line">        self.b = b</span><br><span class="line">        self.db = db</span><br></pre></td></tr></table></figure>
<h3 id="linear">Linear</h3>
<p>定义全连接层, 全连接层可以实现网络空间大小的放缩，全连接层是实现深网的一种方式(但不推荐，深网的构建可以使用残差网络或者高速网络来构建)</p>
<p><img src="/images/posts/simnet/02.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(NN)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">                 dim_in, </span></span></span><br><span class="line"><span class="function"><span class="params">                 dim_out, </span></span></span><br><span class="line"><span class="function"><span class="params">                 init=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                 pretrained=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 zero_bias=False)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> isinstance(pretrained, tuple):</span><br><span class="line">            self.wt, self.b = pretrained</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(init, Init):</span><br><span class="line">               init = ini.Random([dim_in, dim_out])</span><br><span class="line">            self.wt = init()</span><br><span class="line">            <span class="keyword">if</span> zero_bias:</span><br><span class="line">                self.b = ini.Zero([dim_out])()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.b = ini.Random([dim_out])()</span><br><span class="line"></span><br><span class="line">        self.input = <span class="keyword">None</span></span><br><span class="line">        self.output = <span class="keyword">None</span></span><br><span class="line">        self.dw = ini.Zero(self.wt.shape)()</span><br><span class="line">        self.db = ini.Zero([dim_out])()</span><br><span class="line">        self.variable = Variable(self.wt, self.dw, self.b, self.db)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.variable</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.input = args[<span class="number">0</span>]</span><br><span class="line">        self.output = np.dot(self.input, self.wt) + self.b</span><br><span class="line">        <span class="keyword">return</span> self.output</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        self.db = grad</span><br><span class="line">        self.dw += np.dot(self.input.T, grad)</span><br><span class="line">        grad = np.dot(grad, self.wt.T)</span><br><span class="line">        <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h3 id="sigmoid的实现">Sigmoid的实现</h3>
<p>Sigmoid的前向推导很容易实现，也就是将输入放入sigmoid函数即可，难点在Sigmoid的求导上，以下是sigmoid的求导过程</p>
<p><span class="math display">\[\begin{align*}
f{&#39;}(z) &amp; = (\frac{1}{1+e^{-z}}){&#39;} \\\\
&amp; = \frac{e^{-z}}{(1+e^{-z})^{2}} \\\\
&amp; = \frac{1+e^{-z}-1}{(1+e^{-z})^{2}} \\\\
&amp; = \frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}}) \\\\
&amp; = f(z)(1-f(z))
\end{align*} \tag{9}\]</span></p>
<p>知道了前向推导和反向推导的结果就可以用代码实现了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(NN)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Sigmoid, self).__init__()</span><br><span class="line">        self.input = <span class="keyword">None</span></span><br><span class="line">        self.output = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.input = args[<span class="number">0</span>]</span><br><span class="line">        self.output = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-self.input))</span><br><span class="line">        <span class="keyword">return</span> self.output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        grad *= self.output*(<span class="number">1.0</span>-self.output)</span><br><span class="line">        <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h3 id="tanh的实现">tanh的实现</h3>
<p>tanh的实现和sigmoid的类似，只要计算出tanh的导函数就可以很容易实现反向传播部分，tanh的求导结果是</p>
<p><span class="math display">\[f(z){&#39;} = 1-(f(z))^{2} \tag{10}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tanh</span><span class="params">(NN)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Tanh, self).__init__()</span><br><span class="line">        self.input = <span class="keyword">None</span></span><br><span class="line">        self.output = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.input = args[<span class="number">0</span>]</span><br><span class="line">        self.output = ((np.exp(self.input) - np.exp(-self.input)) / </span><br><span class="line">                        np.exp(self.input) + np.exp(-self.input))</span><br><span class="line">        <span class="keyword">return</span> self.output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        grad *= <span class="number">1.0</span> - np.power(self.output, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>同理其他模块的实现也类似，主要包括前向传播和反向传播两部分</p>
<h2 id="实例">实例</h2>
<p>以下构造了一个简单的BP网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BPNet</span><span class="params">(nn.NN)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,D_in, D_hidden, D_out)</span>:</span></span><br><span class="line">        super(Mnistnet,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layers = Pipe(</span><br><span class="line">            nn.Linear(D_in, D_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(D_hidden, D_out)</span><br><span class="line">        )</span><br><span class="line">        self.criterion = loss.MSE()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,*args)</span>:</span></span><br><span class="line">        x = args[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.layers.forward(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,grad=None)</span>:</span></span><br><span class="line">        grad=self.criterion.backward(grad)</span><br><span class="line">        self.layers.backward(grad)</span><br></pre></td></tr></table></figure>
<h2 id="可视化训练">可视化训练</h2>
<p>使用simnet实现了一个简单的BP网络，数据集是 <code>mnist</code>，做了可视化训练<code>demo</code> , 效果如下图</p>
<p><img src="/images/posts/simnet/bp.png"></p>
<p>详细的demo地址 <a href="https://github.com/SeanLee97/simnet/tree/master/examples/mnist" target="_blank" rel="noopener">examples/mnist</a></p>
<p>本项目地址<a href="https://github.com/SeanLee97/simnet" target="_blank" rel="noopener">simnet</a></p>
<h2 id="refrence">refrence</h2>
<ul>
<li><a href="https://github.com/guyuchao/BP-Neural-Network" target="_blank" rel="noopener">BP-Neural-Network</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/10/01/常用的梯度下降优化算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/01/常用的梯度下降优化算法/" itemprop="url">常用的梯度下降优化算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-01T10:33:17+08:00">
                2018-10-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/01/常用的梯度下降优化算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/01/常用的梯度下降优化算法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>梯度下降是常用的优化方式，具体的算法有：</p>
<ul>
<li>梯度下降法
<ul>
<li>批梯度下降(Batch Gradient Descent, BGD)</li>
<li>随机梯度下降(Stochastic Gradient Decent, SGD)</li>
<li>小批量梯度下降(Mini-Batch Gradient Decent, MBGD)</li>
</ul></li>
<li>梯度下降优化
<ul>
<li>动量梯度下降(Gradient Descent with Momentum)</li>
<li>均方根支(Root Mean Square Prop, RMSprop)</li>
<li>自适应矩估计(Adaptive Moment Estimation, Adam)</li>
</ul></li>
</ul>
<p>本文主要简单介绍上述优化方法</p>
<h2 id="知识储备">知识储备</h2>
<h3 id="梯度下降法">梯度下降法</h3>
<p>梯度下降法(gradient descent)是求解无约束最优化问题的一种最常用方法，它是一种迭代算法，每一步需要求解目标函数的梯度向量。它的优点是实现简单，缺点是一般情况下不能保证解是全局最优的</p>
<h4 id="导数">导数</h4>
<p><img src="/images/posts/gradient_descent/01.png"></p>
<p><strong>方向导数定义：</strong> 如上图, <span class="math inline">\(p{}&#39;\)</span> 沿着 <span class="math inline">\(l\)</span> 趋于 <span class="math inline">\(p\)</span> 时，如果函数的增量 <span class="math inline">\(f(x+\Delta x, y+\Delta y) - f(x, y)\)</span> 与 <span class="math inline">\(pp{}&#39;\)</span> 两点间的距离 <span class="math inline">\(\rho = \sqrt{(\Delta x)^{2} + (\Delta y)^{2}}\)</span> 的比值的极限存在，则称此极限为 <span class="math inline">\(p\)</span> 沿着 <span class="math inline">\(l\)</span> 方向的导数，记作</p>
<p><span class="math display">\[\frac{\partial f}{\partial l} = \lim_{\rho \rightarrow 0} \frac{f(x+\Delta x, y+\Delta y) - f(x, y)}{\rho } \\\\
\tag{1}\]</span></p>
<p>更一般的，对于函数 <span class="math inline">\(f(x)\)</span> , 在 <span class="math inline">\(x_{0}\)</span> 处的导数为</p>
<p><span class="math display">\[\begin{align*}
f{}&#39;(x_{0}) &amp; = \lim_{x \rightarrow x_{0}} \frac{f(x) - f(x_{0})}{x-x_{0}} \\\\
&amp; = \lim_{\Delta x \rightarrow 0} \frac{f(x_{0}+\Delta x) - f(x_{0})}{\Delta x} \\\\
\tag{2}
\end{align*}\]</span></p>
<h4 id="梯度">梯度</h4>
<p>函数在某点的梯度的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值</p>
<p><strong>定义：</strong> 设函数 <span class="math inline">\(z = f(x, y)\)</span> 在平面区域D内，具有<strong>一阶连续偏导数</strong>，则对于每一点 <span class="math inline">\(p(x, y) \in D\)</span> , 都可定义一个向量 <span class="math inline">\(\frac{\partial f}{\partial x} \underset{i}{\rightarrow} + \frac{\partial f}{\partial y} \underset{j}{\rightarrow}\)</span> 满足梯度的条件，则这向量称为 <span class="math inline">\(z=f(x, y)\)</span> 在点 <span class="math inline">\(p(x, y)\)</span> 的梯度，记作</p>
<p><span class="math display">\[\begin{align*}gradf(x, y) = \frac{\partial f}{\partial x} \underset{i}{\rightarrow} + \frac{\partial f}{\partial y} \underset{j}{\rightarrow} \\\\
\tag{3}
\end{align*}\]</span></p>
<h3 id="牛顿法和拟牛顿法">牛顿法和拟牛顿法</h3>
<p>牛顿法和拟牛顿法也是求解无约束最优化问题的常用方法，它的优点是收敛速度快。一般用来求解大规模数据的优化问题</p>
<h4 id="海森矩阵hessian-matrix">海森矩阵(Hessian Matrix)</h4>
<p>海森矩阵是一个<strong>多变量实值函数</strong>的<strong>二阶偏导数</strong>组成的<strong>方块矩阵</strong>,假设有一实数函数 <span class="math inline">\(f(x_{1}, x_{2}, ..., x_{n})\)</span> ，如果 <span class="math inline">\(f\)</span> 所有的二阶偏导数都存在，那么海森矩阵为对称矩阵， <span class="math inline">\(f\)</span> 的海森矩阵的第 <span class="math inline">\(ij\)</span> 项即为 <span class="math inline">\(H(f)_{ij}(x) = D_{i}D_{j}f(x)\)</span> ，其中 <span class="math inline">\(x=(x_{1}, x_{2}, ..., x_{n})\)</span> 即</p>
<p><img src="/images/posts/gradient_descent/02.png"></p>
<p>更一般的海森矩阵也可表示为</p>
<p><span class="math display">\[\begin{align*}
H(x) = \begin{bmatrix}
\frac{\partial ^{2} f}{\partial x_{i} \partial x_{j}}
\end{bmatrix} _{n\times n} \\\\
\tag{4}
\end{align*}\]</span></p>
<h4 id="正定半正定矩阵">正定半正定矩阵</h4>
<ul>
<li>正定矩阵：所有特征值大于0 (&gt;0)</li>
<li>负定矩阵：所有特征值小于0 (&lt;0)</li>
<li>半正定矩阵： 所有特征值为非负（&gt;=0）</li>
<li>半负定矩阵：所有特征值为非正（&lt;=0）</li>
<li>不定矩阵：特征值有正有负</li>
</ul>
<h4 id="牛顿法">牛顿法</h4>
<p>牛顿法是迭代算法，每一步需要求解目标函数的海森矩阵的逆矩阵，计算方法比较复杂，这里不详细叙述，具体可阅读<a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank" rel="noopener">Newton’s method in optimization</a></p>
<h4 id="拟牛顿法">拟牛顿法</h4>
<p>拟牛顿法是通过正定矩阵近似的海森矩阵的逆矩阵来简化牛顿法的计算过程，常用算法有 <code>DFP</code>, <code>BFGS</code></p>
<h3 id="等高线">等高线</h3>
<p>在几何上 <span class="math inline">\(z=f(x, y)\)</span> 表示一个曲面，当曲面被平面 <span class="math inline">\(z = c\)</span> 所截，得到的曲线 <span class="math inline">\(\left\{\begin{matrix} z = f(x, y) \\\\ z = c \end{matrix}\right.\)</span> 在 <span class="math inline">\(xoy\)</span> 面上的投影方程 <span class="math inline">\(f(x, y)=c\)</span> 称为等值线，几何上称为等高线</p>
<p><img src="/images/posts/gradient_descent/03.png"></p>
<h2 id="梯度下降法-1">梯度下降法</h2>
<h3 id="批梯度下降batch-gradient-descent-bgd">批梯度下降(Batch Gradient Descent, BGD)</h3>
<p>批梯度下降法在更新参数时使用所有样本来进行更新</p>
<p><span class="math display">\[\begin{align*}
J(w, b) = \frac{1}{m}\sum_{i=1}^{m} L (\hat{y^{(i)}}, y^{(i)}) + \frac{\lambda }{2m}\sum \left \| w \right \|_{F}^{2} \\\\
w_{j} := w_{j} - \alpha \frac{\partial J(w, b)}{\partial w_{j}} \\\\
b_{j} := b_{j} - \alpha \frac{\partial J(w, b)}{\partial b_{j}} \\\\
\tag{5}
\end{align*}\]</span></p>
<p>其中 <span class="math inline">\(\frac{\lambda }{2m}\sum \left \| w \right \|_{F}^{2}\)</span> 是L2正则项, <span class="math inline">\(\alpha\)</span> 是学习速率 <code>learning rate</code></p>
<h4 id="bgd的梯度下降图">BGD的梯度下降图</h4>
<p><img src="/images/posts/gradient_descent/04.png"></p>
<h4 id="bgd的优缺点">BGD的优缺点</h4>
<ul>
<li>优点：最小化<strong>所有训练样本</strong>的损失函数得到全局最优</li>
<li>缺点：当样本数目很多时，训练过程很慢</li>
</ul>
<h4 id="python伪代码">Python伪代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="comment"># 是对每个epoch的所有数据进行计算的</span></span><br><span class="line">    grad = loss_fn(*args, **kwargs)</span><br><span class="line">    params = params - learning_rate * grad</span><br></pre></td></tr></table></figure>
<h3 id="随机梯度下降stochastic-gradient-decent-sgd">随机梯度下降(Stochastic Gradient Decent, SGD)</h3>
<p>每次通过一个样本来迭代更新</p>
<p><span class="math display">\[\begin{align*}
J(w, b) = L (\hat{y^{(i)}}, y^{(i)}) + \frac{\lambda }{2}\sum \left \| w \right \|_{F}^{2} \\\\
w_{j} := w_{j} - \alpha \frac{\partial J(w, b)}{\partial w_{j}} \\\\
b_{j} := b_{j} - \alpha \frac{\partial J(w, b)}{\partial b_{j}} \\\\
\tag{6}
\end{align*}\]</span></p>
<h4 id="sgd的梯度下降图">SGD的梯度下降图</h4>
<p><img src="/images/posts/gradient_descent/05.png"></p>
<h4 id="sgd的优缺点">SGD的优缺点</h4>
<ul>
<li>优点：训练速度快</li>
<li>缺点：不易找到全局最优</li>
</ul>
<h4 id="python伪代码-1">Python伪代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    shuffle(data)</span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">        grad = loss_fn(*args, **kwarga)</span><br><span class="line">        params = params - learning_rate * grad</span><br></pre></td></tr></table></figure>
<h3 id="小批量梯度下降算法mini-batch-gradient-decent-mbgd">小批量梯度下降算法(Mini-Batch Gradient Decent, MBGD)</h3>
<p>对随机梯度下降和批梯度下降进行了折衷， 每次用 <span class="math inline">\(t (1 &lt; t &lt; m)\)</span> 个样本进行更新</p>
<p><span class="math display">\[\begin{align*}
J(w, b) = \frac{1}{k}\sum_{i=1}^{k} L (\hat{y^{(i)}}, y^{(i)}) + \frac{\lambda }{2k}\sum \left \| w \right \|_{F}^{2} \\\\
w_{j} := w_{j} - \alpha \frac{\partial J(w, b)}{\partial w_{j}} \\\\
b_{j} := b_{j} - \alpha \frac{\partial J(w, b)}{\partial b_{j}} \\\\
\tag{7}
\end{align*}\]</span></p>
<h4 id="mbgd的梯度下降图">MBGD的梯度下降图</h4>
<p><img src="/images/posts/gradient_descent/06.png"></p>
<h4 id="python伪代码-2">Python伪代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    shuffle(data)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> next_batch(data, batch_size):</span><br><span class="line">        grad = loss_fn(*args, **kwargs)</span><br><span class="line">        params = params - learning_rate * grad</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降优化">梯度下降优化</h2>
<p>优化的方式一般有两种：</p>
<ul>
<li>算法</li>
<li>优化方法的选择，比如大数据量下采用牛顿法/拟牛顿法进行优化</li>
</ul>
<h3 id="指数加权平均">指数加权平均</h3>
<p>是一种常用的序列数据处理方式</p>
<p><span class="math display">\[\begin{align*}
S_{t} = \begin{cases}
Y_{1} &amp; \text{ if } t=1\\ 
\beta S_{t-1} + (1-\beta)Y_{t} &amp; \text{ if } t&gt;1 
\end{cases} \\\\
\tag{8}
\end{align*}\]</span></p>
<p><span class="math inline">\(Y_{t}\)</span> 为 <span class="math inline">\(t\)</span>下的实际值， <span class="math inline">\(S_{t}\)</span> 为 <span class="math inline">\(t\)</span> 下加权平均后的值， <span class="math inline">\(\beta\)</span> 为权重</p>
<h3 id="动量梯度下降gradient-descent-with-momentum">动量梯度下降（Gradient Descent with Momentum）</h3>
<p>是计算梯度的指数加权平均数，并利用该值来更新参数值</p>
<p><span class="math display">\[\begin{align*}
&amp; \upsilon_{dw} = \beta \upsilon_{dw} + (1-\beta) dw \\\\
&amp; \upsilon_{db} = \beta \upsilon_{db} + (1-\beta) db \\\\
&amp; w := w - \alpha \upsilon_{dw} \\\\
&amp; b := b - \alpha \upsilon_{db} \\\\
\tag{9}
\end{align*}\]</span></p>
<p>SGD 在局部沟壑中很容易发生振荡，所以在这种情况下下降速度会很慢，而动量能在一定程度上抑制这种震荡，使得SGD的下降更平稳</p>
<p>如下图为不加Momentum和加了Momentum的区别</p>
<img src="/images/posts/gradient_descent/07.gif">
<p align="center">
未加Momentum的SGD
</p>
<img src="/images/posts/gradient_descent/08.gif">
<p align="center">
加了Momentum的SGD
</p>
<p><strong>特点：</strong>当前后梯度方向一致时，Momentum梯度下降能够<strong>加速学习</strong>；前后梯度方向不一致时,Momentum梯度下降能够<strong>抑制震荡</strong></p>
<h3 id="均方根支root-mean-square-prop-rmsprop">均方根支(Root Mean Square Prop, RMSProp)</h3>
<p>在梯度进行指数加权平均的基础上引入了平方和平方根</p>
<p><span class="math display">\[\begin{align*}
&amp; S_{dw} = \beta S_{dw} + (1-\beta) dw^{2} \\\\
&amp; S_{db} = \beta S_{db} + (1-\beta) db^{2} \\\\
&amp; w := w - \alpha \frac{dw}{\sqrt{S_{dw} + \epsilon }} \\\\
&amp; b := b - \alpha \frac{dw}{\sqrt{S_{db} + \epsilon }} \\\\
\tag{10}
\end{align*}\]</span></p>
<p><span class="math inline">\(\epsilon\)</span> 一般值很小，主要是用来提高数值稳定性，防止分母过小</p>
<p><strong>特点：</strong> 当 <span class="math inline">\(dw\)</span> 或 <span class="math inline">\(db\)</span> 较大时，<span class="math inline">\(dw^{2}\)</span> 和 <span class="math inline">\(db^{2}\)</span> 也会较大，因此 <span class="math inline">\(S_{dw}\)</span> <span class="math inline">\(S_{db}\)</span> 也是较大的，最终使得 <span class="math inline">\(\frac{dw}{\sqrt{S_{dw} + \epsilon}}\)</span> <span class="math inline">\(\frac{db}{\sqrt{S_{db} + \epsilon}}\)</span> 较小，这也减少了振荡</p>
<h3 id="自适应矩估计adaptive-moment-estimation-adam">自适应矩估计(Adaptive Moment Estimation, Adam)</h3>
<p>可以认为是 <code>Momentum</code> 和 <code>RMSProp</code> 的结合</p>
<p><span class="math display">\[\begin{align*}
&amp; \upsilon_{dw} = \beta_{1} \upsilon_{dw} + (1-\beta _{1}) dw, \upsilon _{db} = \beta_{1} \upsilon_{db} + (1-\beta _{1}) db \\\\
&amp; S_{dw} = \beta_{2} S_{dw} + (1-\beta _{2}) dw^{2}, S_{db} = \beta_{2} S_{db} + (1-\beta _{2}) db^{2} \\\\
&amp; \upsilon_{dw}^{correct} = \frac{\upsilon _{dw}}{1-\beta_{1}^{t}}, \upsilon_{db}^{correct} = \frac{\upsilon_{db}}{1-\beta_{1}^{t}} \\\\
&amp; S_{dw}^{correct} = \frac{S_{dw}}{1-\beta_{2}^{t}}, S_{db}^{correct} = \frac{S_{db}}{1-\beta_{2}^{t}} \\\\
&amp; w := w - \alpha \frac{\upsilon_{dw}^{correct}}{\sqrt{S_{dw}^{correct}} + \epsilon} \\\\
&amp; b := b - \alpha \frac{\upsilon_{db}^{correct}}{\sqrt{S_{db}^{correct}} + \epsilon} \\\\
\tag{11}
\end{align*}\]</span></p>
<p><span class="math inline">\(\beta _{1}\)</span>为第一阶矩，<span class="math inline">\(\beta _{2}\)</span> 为第二阶矩</p>
<h3 id="各优化算法的比较">各优化算法的比较</h3>
<p><img src="/images/posts/gradient_descent/09.gif"> <img src="/images/posts/gradient_descent/10.gif"></p>
<h2 id="reference">Reference</h2>
<p>[1]李航《统计学习方法》</p>
<p>[2] https://wenku.baidu.com/view/d3dbe40903d8ce2f00662358.html</p>
<p>[3] https://wenku.baidu.com/view/23aca9eab8f67c1cfad6b84f.html</p>
<p>[4] https://zh.wikipedia.org/wiki/%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5</p>
<p>[5] http://binweber.top/2017/10/06/deep_learning_4/</p>
<p>[6] http://ruder.io/optimizing-gradient-descent/</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/09/09/简单理解上帝算法EM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/09/简单理解上帝算法EM/" itemprop="url">简单理解"上帝算法"EM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-09T20:00:07+08:00">
                2018-09-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index">
                    <span itemprop="name">技术日志</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/09/简单理解上帝算法EM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/09/09/简单理解上帝算法EM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>EM(Expectation Maximization, 期望最大)算法也称“上帝算法”，能称为“上帝”可见它有多牛掰。</p>
<p>EM的原理很简单但真正理解起来是挺痛苦的一件事，本文以凡人视角尝试理解上帝本意。</p>
<h2 id="知识储备">知识储备</h2>
<h3 id="kl距离">KL距离</h3>
<p>KL距离又称相对熵(relative entropy)，用来衡量空间上两个概率分布 <span class="math inline">\(P(x)\)</span>，<span class="math inline">\(Q(x)\)</span> 的相对差距的测度（通俗的说就是两个概率分布的距离）</p>
<p><span class="math display">\[\begin{align*}
D_{KL}(P||Q) &amp; = \sum_{i} P(i)log\ \frac{P(i)}{Q(i)} \\
&amp; = \sum_{i} P(i) log\ P(i) - \sum_{i} P(i) log\  Q(i) \\
&amp; = E_{P}(log\ P(i)) - E_{P}(log\ Q(i))
\end{align*} \tag{1}\]</span></p>
<p><span class="math inline">\((1)\)</span> 式便于理解“距离”的概念，因为A, B两点的距离我们一般表示为 <span class="math inline">\(|A-B|\)</span> ，当 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 是同概率分布时KL距离为0</p>
<h3 id="极大似然估计">极大似然估计</h3>
<p>极大似然估计，是用来估计一个概率模型的参数的一种方法，它一般求解似然函数</p>
<p><span class="math display">\[L(\theta | x_{1}...x_{m}) = f_{\theta}(x_{1}...x_{m})\]</span></p>
<p>取最大值时的参数 <span class="math inline">\(\widehat{\theta}\)</span></p>
<p><span class="math display">\[\widehat{\theta} = \underset{\theta}{argmax} L(\theta | x_{1}...x_{m})\]</span></p>
<h3 id="隐变量">隐变量</h3>
<p>在统计学中，隐变量或潜变量指的是不可观测的随机变量,隐变量可以通过使用数学模型依据观测得的数据被推断出来。</p>
<p>通俗的说隐变量是存在的但是你无法看到。举个例子吧，分类和聚类看起来是差不多的，都是把一堆相似的东西分到一起，但是分类（显式）提供了类别信息，而聚类没有（显式）提供类别信息，聚类之所以能把一堆相似的东西分到一起可以认为它内部定义了一个“隐藏”的类别信息，可以认为是隐变量</p>
<p>（上述解释系本人捏造的，不一定正确）</p>
<h3 id="jensen-不等式">Jensen 不等式</h3>
<p><img src="/images/posts/em/jensen.jpg"></p>
<p>如果 <span class="math inline">\(f\)</span> 是凸函数(如上图)， <span class="math inline">\(x\)</span> 是随机变量有</p>
<p><span class="math display">\[E(f(x)) \geq f(E(x)) \tag{2}\]</span></p>
<p>凹函数则相反</p>
<p><span class="math display">\[E(f(x)) \leq f(E(x))\]</span></p>
<p><strong>当 <span class="math inline">\(x\)</span> 是常量时上式取等</strong>，即上图<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>两点重合时取等</p>
<p>Jensen不等式在下述EM算法的推导中很重要</p>
<h4 id="凹凸函数">凹凸函数</h4>
<p>可能受中文<code>凹凸</code>字型的影响会对<code>凹凸</code>函数有歧义，凸函数可理解为<code>下凸</code>，凹函数可理解为<code>上凸</code></p>
<p><img src="/images/posts/em/aotufunc.png"></p>
<h2 id="em算法的推导">EM算法的推导</h2>
<p>EM算法主要分两个求解步骤，称为E步和M步</p>
<h3 id="e步">E步</h3>
<p>假设有训练集</p>
<p><span class="math display">\[{x^{1}, ...., x^{m}}\]</span></p>
<p>包含 <span class="math inline">\(m\)</span> 个独立样本，我们希望从中找到该组数据的模型 <span class="math inline">\(p(x, z)\)</span> 的参数</p>
<p>在这里我们先以<strong>极大似然</strong>的思想去建立目标函数，取对数似然函数</p>
<p><span class="math display">\[L(\theta) = \sum_{i=1}^{m} log\ p(x; \theta) \tag{3}\]</span></p>
<p>我们给 <span class="math inline">\((3)\)</span> 式加入隐变量 <span class="math inline">\(z\)</span> , 对于联合分布 <span class="math inline">\(p(x, z; \theta)\)</span> 有 <span class="math inline">\(\sum_{z}\ p(x, z; \theta) = p(x; \theta)\)</span>, 故有</p>
<p><span class="math display">\[
\begin{align*}
L(\theta) &amp; = \sum_{i=1}^{m} log\ p(x; \theta) \\
&amp; = \sum_{i=1}^{m} log\ \sum_{z} p(x, z; \theta)
\end{align*} \tag{4}
\]</span></p>
<p><span class="math inline">\(z\)</span> 是随机的隐变量，直接找到参数的估计是很困难的，我们要做的是建立 <span class="math inline">\(L(\theta)\)</span> 的下界，然后<strong>求该下界的最大值，重复求解直至收敛到局部最大值</strong></p>
<p>因为假设了样本是独立的，故整体的概率有</p>
<p><span class="math display">\[P = \prod_{i} p(x^{(i)}; \theta) = \prod_{i} \sum_{z^{(i)}}  p(x^{(i)}, z^{(i)}; \theta)\]</span></p>
<p>为了方便推导，一般两边取对数，故有</p>
<p><span class="math display">\[\begin{align*}
log\ P &amp; = \sum_{i} log\ p(x^{(i)}; \theta) \\
&amp; = \sum_{i} log\  \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta)
\end{align*} \tag{5}\]</span></p>
<p>现在我们假设 <span class="math inline">\(Q_{i}\)</span> 是 <span class="math inline">\(z\)</span> 的某一分布，<span class="math inline">\(Q_{i} \geqslant 0\)</span>， <span class="math inline">\(\sum_{i} Q_{i} = 1\)</span>，此时 <span class="math inline">\((5)\)</span> 有</p>
<p><span class="math display">\[\begin{align*}
log\ P &amp; = \sum_{i} log\ p(x^{(i)}; \theta) \\
&amp; = \sum_{i} log\  \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
&amp; = \sum_{i} log\  \sum_{z^{(i)}} Q_{i}(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}
\end{align*} \tag{6}\]</span></p>
<p>对于 <span class="math inline">\((6)\)</span> 有</p>
<p><span class="math display">\[\sum_{i} log\  \sum_{z^{(i)}} Q_{i}(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} = \sum_{i} log(E_{Q_{i}} (\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}) ) \tag{7}\]</span></p>
<p>因为<code>log(x)</code>是凹函数，根据<strong>Jensen不等式</strong>，对于<span class="math inline">\((7)\)</span> 有</p>
<p><span class="math display">\[\begin{align*}
\sum_{i} log(E_{Q_{i}} ( \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}) )
\geq \sum_{i} E_{Q_{i}}(log( \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}) )
=\sum_{i} \sum_{z^{(i)}} Q_{i}(z^{(i)}) log\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} 
\end{align*} \tag{8}\]</span></p>
<p>因此由<span class="math inline">\((8)\)</span>可得</p>
<p><span class="math display">\[\begin{align*}
log\ P &amp; = \sum_{i} log\ p(x^{(i)}; \theta) \\
&amp; = \sum_{i} log\  \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
&amp; = \sum_{i} log\  \sum_{z^{(i)}} Q_{i}(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} \\
&amp; \geq \sum_{i} \sum_{z^{(i)}} Q_{i}(z^{(i)}) log\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} 
\end{align*} \tag{9}\]</span></p>
<p>因为之前已经说了我们要求最大下界，回到Jensen不等式 <span class="math inline">\((2)\)</span> 当x为常量时才能取等号，也就是对于 <span class="math inline">\((8)\)</span> 当</p>
<p><span class="math display">\[\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} = c \tag{10}\]</span></p>
<p><span class="math inline">\(c\)</span> 为常数时才能取等，从<strong>KL距离</strong>解释就是当<span class="math inline">\(p\)</span> 和 <span class="math inline">\(Q\)</span> 的KL距离为0时才能取等，也就是说 <span class="math inline">\(p\)</span> 和 <span class="math inline">\(Q\)</span> 是同分布的，而由 <span class="math inline">\((10)\)</span>可知 <strong><span class="math inline">\(p\)</span> 和 <span class="math inline">\(Q\)</span> 是线性关系</strong>，故属于同分布, 对<span class="math inline">\(p\)</span>归一化有</p>
<p><span class="math display">\[\frac{p(x^{(i)}, z^{(i)}; \theta)}{\sum_{z} p(x^{(i)}, z; \theta)} = 1 \tag{11}\]</span></p>
<p>此时刚好满足 <span class="math inline">\(\sum_{z} Q_{i}(z^{(i)}) = 1\)</span>，因此我们可以用 <span class="math inline">\(p\)</span> 来表示 <span class="math inline">\(Q\)</span></p>
<p><span class="math display">\[\begin{align*}
Q_{i}(z^{(i)}) &amp; = \frac{p(x^{(i)}, z^{(i)}; \theta)}{\sum_{z} p(x^{(i)}, z; \theta)} \\
&amp; = \frac{p(x^{(i)}, z^{(i)}; \theta)}{p(x^{(i)}; \theta)} \\
&amp; = p(z^{(i)} | x^{(i)}; \theta)
\end{align*} \tag{12}\]</span></p>
<p>由<span class="math inline">\((12)\)</span>可知，<span class="math inline">\(Q_{i}(z^{(i)})\)</span> 的估计就是给定 <span class="math inline">\(x^{(i)}\)</span> 下 <span class="math inline">\(z^{(i)}\)</span> 发生的条件分布，此时可以使得 <span class="math inline">\((9)\)</span> 式取等</p>
<p>因此<strong>E步可以认为是固定参数 <span class="math inline">\(\theta\)</span> 最大化 <span class="math inline">\(Q_{i}(z^{(i)})\)</span></strong></p>
<h3 id="m步">M步</h3>
<p><strong>M步的思想是固定住 <span class="math inline">\(Q_{i}(z^{(i)})\)</span> 最大化 <span class="math inline">\(\theta\)</span></strong>，即</p>
<p><span class="math display">\[\theta := \underset{\theta}{argmax} \sum_{i} \sum_{z^{(i)}} Q_{i}(z^{(i)}) log\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} \tag{13}\]</span></p>
<p>至于M步如何求解就是EM算法计算的难点所在，此文不做详细讨论</p>
<h3 id="综述">综述</h3>
<p>由上可知EM算法，就是通过E步和M步来循环更新 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(Q\)</span> 使得极值推向设定的收敛值</p>
<h2 id="gmmgaussian-mixed-model-高斯混合模型与em">GMM(Gaussian Mixed Model) 高斯混合模型与EM</h2>
<p>高斯混合模型是指多个高斯分布的混合 <img src="/images/posts/em/gmm.png"></p>
<h3 id="一个经典的高斯混合模型的例子">一个经典的高斯混合模型的例子</h3>
<p>随机挑选10000个志愿者，然后测量他们的身高，若样本中存在男性和女性，且他们的身高分别服从 <span class="math inline">\(N(\mu_{1}, \sigma_{1})\)</span> 和 <span class="math inline">\(N(\mu_{2}, \sigma_{2})\)</span> 的分布，在性别未知的情况下试估计参数 <span class="math inline">\(\mu_{1}\)</span>、<span class="math inline">\(\sigma_{1}\)</span>、<span class="math inline">\(\mu_{2}\)</span>、<span class="math inline">\(\sigma_{2}\)</span></p>
<p>由于性别未知，用常规的方法是无法求得男女身高对应的高斯分布的，一般用EM算法求解</p>
<p>在将EM算法求解GMM前，先要知道高斯分布的极大似然估计</p>
<h3 id="高斯分布的极大似然估计">高斯分布的极大似然估计</h3>
<p>假定给定一组样本 <span class="math inline">\(x_{1}, x_{2}, ..., x_{n}\)</span> ，已知他们来自于高斯分布 <span class="math inline">\(N(\mu, \sigma)\)</span> ，试估计参数 <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span></p>
<p>对数似然估计有</p>
<p><span class="math display">\[\begin{align*}
L(x) &amp; = \sum_{i} log \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}} \\
&amp; = (\sum_{i} log \frac{1}{\sqrt{2\pi} \sigma}) + (\sum_{i} -\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}) \\
&amp; = -\frac{n}{2} log\ 2\pi \sigma^{2} - \frac{1}{2\sigma^{2}} \sum_{i}(x_{i} - \mu)^{2}
\end{align*} \tag{14}\]</span></p>
<p>对 <span class="math inline">\(L(x)\)</span> 求偏导有</p>
<p><span class="math display">\[\frac{\partial L(x)}{\partial \mu} \Rightarrow \mu = \frac{1}{n} \sum_{i} x_{i} \tag{15}\]</span></p>
<p><span class="math display">\[\frac{\partial L(x)}{\partial \sigma^{2}} \Rightarrow \sigma^{2} = \frac{1}{n}\sum_{i} (x_{i}-\mu)^{2} \tag{16}\]</span></p>
<p>可知极大似然估计使用了样本均值和样本伪方差（样本方差的分母是 <span class="math inline">\(n-1\)</span>, 这里是 <span class="math inline">\(n\)</span>）来估计参数</p>
<h3 id="gmm">GMM</h3>
<p>随机变量 <span class="math inline">\(X\)</span> 是由k个高斯分布混合而成，取各个高斯分布的概率为 <span class="math inline">\(\pi_{1}, \pi_{2}, ..., \pi_{k}\)</span> 第 <span class="math inline">\(i\)</span> 个高斯分布的均值为 <span class="math inline">\(\mu_{i}\)</span> ，方差为 <span class="math inline">\(\Sigma_{i}\)</span> 。若观测到随机变量的一系列样本 <span class="math inline">\(x_{1}, x_{2}, ..., x_{n}\)</span> ,试估计参数 <span class="math inline">\(\underset{\pi}{\rightarrow} (\pi_{1}, \pi_{2}, ..., \pi_{k})\)</span> 、<span class="math inline">\(\underset{\mu}{\rightarrow} (\mu_{1}, \mu_{2}, ..., \mu_{k})\)</span> 和 <span class="math inline">\(\underset{\Sigma}{\rightarrow} (\Sigma_{1}, \Sigma_{2}, ..., \Sigma_{k})\)</span></p>
<p>对上述问题先建立对数似然函数</p>
<p><span class="math display">\[L_{\pi, \mu, \Sigma} (x) = \sum_{i=1}^{n}log(\sum_{k=1}^{K} \pi_{k} N(x_{i} | \mu_{k}, \Sigma_{k})) \tag{17}\]</span></p>
<p>由于在对数函数里有加和一般是不以直接求偏导的，故采用EM算法取估计参数</p>
<h4 id="e步-1">E步</h4>
<p>E步主要估计数据来自哪个组份(E步假设待估计参数是固定的)：对于每个样本 <span class="math inline">\(x_{i}\)</span> 它由<strong>第k个组份生成的概率</strong>为：</p>
<p><span class="math display">\[\gamma(i, k) = \frac{\pi_{k} N(x_{i}|\mu_{k}, \Sigma_{k})}{\sum_{j=1}^{k} \pi_{j}N(X_{i} | \mu_{j}, \Sigma_{j})} \tag{18}\]</span></p>
<p>#### M步 对于所有样本点，对于组份k而言，可看做生成了 { <span class="math inline">\(\gamma(i, k)x_{i} | i=1, 2, ..., N\)</span> } 这些点，组分k是一个标准的高斯分布，由 <span class="math inline">\((15), (16)\)</span> 可得</p>
<p><span class="math display">\[\left\{\begin{matrix}
N_{k} = \sum_{i=1}^{n}\gamma (i, k) \\ 
\mu_{k} = \frac{1}{N_{k}} \sum_{i=1}^{n}\gamma (i, k) x_{i} \\ 
\Sigma_{k} = \frac{1}{N_{k}} \sum_{i=1}^{n}\gamma (i, k)(x_{i} - \mu_{i})^{2} \\ 
\pi_{k} = \frac{N_{k}}{n} = \frac{1}{n} \sum_{i}^{n}\gamma (i, k)
\end{matrix}\right. \tag{19}\]</span></p>
<h2 id="refrence">Refrence</h2>
<p>[0] https://zh.wikipedia.org/wiki/%E9%9A%90%E5%8F%98%E9%87%8F</p>
<p>[1] 李航 《统计学习方法》</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/09/01/SVD的原理及LSA的求解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/01/SVD的原理及LSA的求解/" itemprop="url">SVD的原理及LSA的求解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-01T19:41:55+08:00">
                2018-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index">
                    <span itemprop="name">技术日志</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/01/SVD的原理及LSA的求解/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/09/01/SVD的原理及LSA的求解/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前介绍PCA时提到了SVD，本文主要介绍SVD基本概念以及在NLP上的应用。</p>
<h3 id="svd的定义">SVD的定义</h3>
<p>SVD(Singular Value Decomposition)即奇异值分解，奇异值分解区别于特征值分解可用来分解非方阵矩阵问题。假设矩阵 <span class="math inline">\(A \in R^{m\times n}\)</span> 是一个 <span class="math inline">\(m\times n\)</span> 的矩阵，则它的奇异值分解可分解为三个矩阵：</p>
<p><span class="math display">\[A_{m\times n} = U_{m\times m} \Sigma _{m\times n} V^{T}_{n\times n} \tag{1}\]</span></p>
<p>其中 <span class="math inline">\(U\)</span> 和 <span class="math inline">\(V^{T}\)</span> 称为酉矩阵, <span class="math inline">\(\Sigma\)</span> 称为奇异矩阵，它的形式一般为:</p>
<p><span class="math display">\[\Sigma = \begin{bmatrix}
\sigma_{1} &amp; 0 &amp;  ... &amp; 0\\ 
0 &amp; \sigma_{i} &amp; ... &amp; ...\\ 
... &amp; 0 &amp; \sigma_{k} &amp; ...\\
0 &amp; ... &amp; 0 &amp; ...\\
... &amp; 0 &amp; ... &amp; 0 \\  
\end{bmatrix}\]</span></p>
<p><span class="math inline">\(\lambda_{1} ... \lambda _{m}\)</span> 称为奇异值</p>
<p>更一般的我们更希望奇异矩阵是方阵，因此矩阵<span class="math inline">\(A\)</span>可近似分解为</p>
<p><span class="math display">\[A_{m\times n} \approx  U_{m\times k} \Sigma _{k \times k} V^{T}_{k\times n} \tag{2}\]</span></p>
<p>此时奇异矩阵 <span class="math inline">\(\Sigma\)</span> 有</p>
<p><span class="math display">\[\Sigma = \begin{bmatrix}
\sigma_{1} &amp; ... &amp;  0\\ 
0 &amp; \sigma_{i} &amp; ... \\ 
... &amp; 0 &amp; \sigma_{k} \\
\end{bmatrix}\]</span></p>
<p>看起来是不是和特征值分解的对角矩阵很类似，<strong>非方阵的PCA降维原理就是利用这个奇异矩阵进行的</strong></p>
<p>关于PCA请查阅另一篇文章 <a href="https://seanlee97.github.io/2018/03/29/%E4%BB%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E5%8E%BB%E7%90%86%E8%A7%A3PCA/">从特征值特征向量去理解PCA</a></p>
<h3 id="svd的求解">SVD的求解</h3>
<p>SVD的求解还是要回到<strong>方阵</strong>求解上，也就是要回到<strong>特征值特征向量上</strong></p>
<p>可知 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(m\times n\)</span>矩阵，那么 <span class="math inline">\(A^{T} \in R^{n\times m}\)</span>，则有两种情况可构成方阵：</p>
<ul>
<li><span class="math inline">\(AA^{T} \in R^{m\times m}\)</span></li>
<li><span class="math inline">\(A^{T}A \in R^{n\times n}\)</span></li>
</ul>
<p>由 <span class="math inline">\((1)\)</span> 可知, <span class="math inline">\(U \in R^{m\times m}\)</span>，此时我们用 <span class="math inline">\(AA^{T} \in R^{m\times m}\)</span> 来求解，得到特征值特征向量如下：</p>
<p><span class="math display">\[(AA^{T})u_{i} = \lambda_{i} u_{i} \tag{3}\]</span></p>
<p>计算可求得m个特征值和m个特征向量<span class="math inline">\(u\)</span>，这些特征向量称为A的左奇异向量,酉矩阵 <span class="math inline">\(U = [u_{1}...u_{m}]\)</span></p>
<p>同理，<span class="math inline">\(V^{T} \in R^{n\times n}\)</span>，可用 <span class="math inline">\(A^{T}A\)</span> 来求解</p>
<p><span class="math display">\[(A^{T}A) v_{i} = \lambda_{i} v_{i} \tag{4}\]</span></p>
<p>计算可求得n个特征值和n个特征向量<span class="math inline">\(v\)</span>，这些特征向量称为A的右奇异向量,酉矩阵 <span class="math inline">\(V = [v_{1}...v_{n}]\)</span></p>
<p>对于 <span class="math inline">\(\Sigma = [\sigma_{1} ... \sigma_{k}]\)</span> 的求解有</p>
<p><span class="math display">\[\begin{align*}
&amp; A = U\Sigma V^{T} \\\\
&amp; \Rightarrow AV = U\Sigma \\\\
&amp; \Rightarrow Av_{i} = u_{i}\sigma_{i} \\\\
&amp; \Rightarrow \sigma_{i} = A(v_{i} / u_{i})
\end{align*}\]</span></p>
<p>因此奇异值 <span class="math inline">\(\sigma_{i}\)</span> 是由对应的左右奇异向量 <span class="math inline">\(u_{i}\)</span> 和 <span class="math inline">\(v_{i}\)</span> 求得的</p>
<h3 id="lsa的原理与示例">LSA的原理与示例</h3>
<p>NLP中PCA和LSA的原理就是SVD，PCA上面已经做了简要介绍，这里主要说说LSA</p>
<p>LSA(Latent Semantic Analysis) 潜在语义分析的原理就是 SVD，它可以同时完成三个任务</p>
<ul>
<li>近义词分类 (利用的是 <span class="math inline">\(U\)</span> 矩阵)</li>
<li>主题（文档）分类 (利用的是 <span class="math inline">\(V^{T}\)</span> 矩阵)</li>
<li>类词和主题的相关性 (利用的是 <span class="math inline">\(\Sigma\)</span>)</li>
</ul>
<p><img src="/images/posts/svd/lsa.png"></p>
<p>LSA 的核心思想是<strong>将词和文档映射到潜在语义空间</strong>，再比较其相似性。</p>
<p>下面举个实例讲解，如图有</p>
<h4 id="矩阵-x">矩阵 <span class="math inline">\(X\)</span></h4>
<p>矩阵 <span class="math inline">\(X\)</span> 代表词表到文档的矩阵，其中<strong>每行</strong>代表词典中的词，<strong>每一列</strong>代表一篇文档，<span class="math inline">\(X_{ij}=k\)</span> 代表第 <span class="math inline">\(j\)</span> 篇文档中出现了 <span class="math inline">\(k\)</span> 次词 <span class="math inline">\(i\)</span> （也可用TF-IDF代替），可知该矩阵是不包含位置信息的</p>
<p>假设现在有四个文档： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">doc1 = <span class="string">"""计算机科学是系统性研究信息与计算的理论基础以及它们在计算机系统中如何实现与应用的实用技术的学科"""</span></span><br><span class="line">    </span><br><span class="line">doc2 = <span class="string">"""自然语言处理是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。</span></span><br><span class="line"><span class="string">自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。"""</span></span><br><span class="line"></span><br><span class="line">doc3 = <span class="string">"""人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，</span></span><br><span class="line"><span class="string">该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等"""</span></span><br><span class="line"></span><br><span class="line">doc4 = <span class="string">"""《瓦尔登湖》是美国作家梭罗独居瓦尔登湖畔的记录，描绘了他两年多时间里的所见、所闻和所思。</span></span><br><span class="line"><span class="string">该书崇尚简朴生活，热爱大自然的风光，内容丰厚，意义深远，语言生动"""</span></span><br></pre></td></tr></table></figure></p>
<p>对每个文档进行分词，去停用词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(self, docs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        doc = doc.strip()</span><br><span class="line">        <span class="comment"># 为了简单仅仅保留词的长度大于1的</span></span><br><span class="line">        words = list(filter(<span class="keyword">lambda</span> x: len(x) &gt; <span class="number">1</span>, self.tokenizer(doc))) </span><br><span class="line">        self.docs.append(words)</span><br><span class="line">        self.vocabs.update(words)</span><br><span class="line"></span><br><span class="line">    self.vocabs = list(self.vocabs)</span><br><span class="line">    self.word2idx = dict(zip(self.vocabs, range(len(self.vocabs))))</span><br><span class="line">    </span><br><span class="line">    print(self.vocabs)</span><br></pre></td></tr></table></figure>
<p>输出词典得到： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;方式&apos;, &apos;基础&apos;, &apos;美国作家&apos;, &apos;生动&apos;, &apos;易于&apos;, &apos;人工智能&apos;, &apos;语言&apos;, &apos;计算机系统&apos;, &apos;语言学&apos;, &apos;实质&apos;, &apos;瓦尔登湖&apos;, &apos;生活&apos;, &apos;计算机&apos;, &apos;机器人&apos;, &apos;独居&apos;, &apos;相似&apos;, &apos;人类&apos;, &apos;内容&apos;, &apos;电脑&apos;, &apos;实现&apos;, &apos;所闻&apos;, &apos;计算机程序&apos;, &apos;系统&apos;, &apos;理解&apos;, &apos;理论&apos;, &apos;探讨&apos;, &apos;该书&apos;, &apos;它们&apos;, &apos;数据&apos;, &apos;了解&apos;, &apos;处理&apos;, &apos;企图&apos;, &apos;以及&apos;, &apos;学科&apos;, &apos;生产&apos;, &apos;包括&apos;, &apos;图像识别&apos;, &apos;两年&apos;, &apos;分支&apos;, &apos;一种&apos;, &apos;机器&apos;, &apos;如何&apos;, &apos;风光&apos;, &apos;丰厚&apos;, &apos;描绘&apos;, &apos;认知&apos;, &apos;记录&apos;, &apos;简朴&apos;, &apos;应用&apos;, &apos;智能&apos;, &apos;做出&apos;, &apos;时间&apos;, &apos;自然语言&apos;, &apos;所思&apos;, &apos;崇尚&apos;, &apos;系统性&apos;, &apos;识别&apos;, &apos;梭罗&apos;, &apos;形式&apos;, &apos;大自然&apos;, &apos;深远&apos;, &apos;计算&apos;, &apos;信息&apos;, &apos;意义&apos;, &apos;专家系统&apos;, &apos;生成&apos;, &apos;所见&apos;, &apos;研究&apos;, &apos;一个&apos;, &apos;运用&apos;, &apos;转化&apos;, &apos;领域&apos;, &apos;实用技术&apos;, &apos;计算机科学&apos;, &apos;热爱&apos;, &apos;反应&apos;</span><br></pre></td></tr></table></figure></p>
<p>构造词到文档的矩阵,在这里实现了词袋模型和TF-IDF</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用词袋特征来做权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_bow_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">    matrix = np.zeros([len(self.vocabs), len(self.docs)])</span><br><span class="line">    <span class="keyword">for</span> docidx, words <span class="keyword">in</span> enumerate(self.docs):</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            matrix[self.word2idx[word], docidx] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用tf-idf来做权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_tfidf_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">    tf = self.build_bow_matrix()</span><br><span class="line">    df = np.ones([len(self.vocabs), <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> docidx, words <span class="keyword">in</span> enumerate(self.docs):</span><br><span class="line">        tf[:, docidx] /= np.max(tf[:, docidx])</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            df[self.word2idx[word], <span class="number">0</span>] += <span class="number">1</span></span><br><span class="line">    idf = np.log(len(self.docs)) - np.log(df)</span><br><span class="line">    <span class="keyword">return</span> tf*idf</span><br></pre></td></tr></table></figure>
<p>得到的矩阵为</p>
<p><strong>词袋模型</strong> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0. 1.]</span><br><span class="line"> [0. 1. 1. 1.]</span><br><span class="line"> [0. 2. 0. 0.]</span><br><span class="line"> .............</span><br><span class="line"> [0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 1. 0.]</span><br><span class="line"> [1. 0. 0. 0.]]</span><br><span class="line"> </span><br><span class="line"> shape = (vocab_size, doc_size)</span><br></pre></td></tr></table></figure></p>
<p><strong>TF-IDF</strong> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.          0.          0.          0.34657359]</span><br><span class="line"> [ 0.          0.          0.          0.        ]</span><br><span class="line"> [ 0.          0.08219488  0.          0.        ]</span><br><span class="line"> .....</span><br><span class="line"> [ 0.          0.          0.          0.34657359]</span><br><span class="line"> [ 0.          0.          0.23104906  0.        ]</span><br><span class="line"> [ 0.69314718  0.          0.          0.        ]]</span><br><span class="line"> </span><br><span class="line"> shape = (vocab_size, doc_size)</span><br></pre></td></tr></table></figure></p>
<h4 id="u-矩阵的应用"><span class="math inline">\(U\)</span> 矩阵的应用</h4>
<p>分解得到的 <span class="math inline">\(U \in R^{m\times m}\)</span> 矩阵<strong>每一行</strong>代表一类近义词,我们一般对每一行的权值进行逆序排序获得top k个权值最大的下标，这些下标对应的词即为近义词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sim_words</span><span class="params">(self, k=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.kernel == <span class="string">'tfidf'</span>:</span><br><span class="line">            matrix = self.build_tfidf_matrix()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            matrix = self.build_bow_matrix()</span><br><span class="line"></span><br><span class="line">        U, S, Vt = np.linalg.svd(matrix)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对权值逆序排序</span></span><br><span class="line">        sort_idx = np.argsort(-U)</span><br><span class="line">        <span class="comment"># 一般不取第一列，第一列的词往往是本身</span></span><br><span class="line">        topk = sort_idx[:, <span class="number">1</span>:k+<span class="number">1</span>] </span><br><span class="line">        print(<span class="string">"word \t similarity"</span>)</span><br><span class="line">        <span class="keyword">for</span> widx, word <span class="keyword">in</span> enumerate(self.vocabs):</span><br><span class="line">            line = word + <span class="string">":\t"</span></span><br><span class="line">            idxs = topk[widx]</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> idxs:</span><br><span class="line">                line += str(self.vocabs[idx]) + <span class="string">" "</span></span><br><span class="line">            print(line)</span><br></pre></td></tr></table></figure>
<p>得到的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">计算机系统:	自然语言 领域 如何</span><br><span class="line">数据:	智能 计算机科学 研究</span><br><span class="line">计算机:	智能 计算机科学 研究</span><br><span class="line">语言:	如何 处理 计算</span><br></pre></td></tr></table></figure>
<p>由于只是作为演示用，文档太少，不能得到较好的效果，可用大文本尝试一下</p>
<h4 id="vt-矩阵的应用"><span class="math inline">\(V^{T}\)</span> 矩阵的应用</h4>
<p><span class="math inline">\(V^{T} \in R^{n\times n}\)</span> 矩阵<strong>每一列</strong>代表一个主题，<strong>列中每一行</strong>代表主题相关文档，我们对每一列的权值进行逆序，得到对应下标，这样每一列对应的下标代表同一类别的相关文档下标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topic_relate</span><span class="params">(self, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.kernel == <span class="string">'tfidf'</span>:</span><br><span class="line">            matrix = self.build_tfidf_matrix()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            matrix = self.build_bow_matrix()</span><br><span class="line"></span><br><span class="line">        U, S, Vt = np.linalg.svd(matrix)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 按列逆序排序</span></span><br><span class="line">        sort_idx = np.argsort(-Vt, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 一般不取第一行，第一行是自己本身</span></span><br><span class="line">        topk = sort_idx[<span class="number">1</span>:k+<span class="number">1</span>, :]</span><br><span class="line">        print(topk)</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[2 3 0 1]</span><br><span class="line"> [2 0 1 3]]</span><br></pre></td></tr></table></figure>
<h4 id="sigma-矩阵的作用"><span class="math inline">\(\Sigma\)</span> 矩阵的作用</h4>
<p><span class="math inline">\(\Sigma \in R^{m\times n}\)</span> 代表类词和文章类之间的相关性</p>
<p>由此可见LSA可谓一举三得啊！</p>
<h4 id="lsa的缺点及改进">LSA的缺点及改进</h4>
<p>LSA的缺点在于采用暴力SVD矩阵分解，如果维数大了，矩阵大了就难以计算了，加上SVD的分布式计算又是很难实现的，所以在大规模文档中可能不用直接用LSA</p>
<h5 id="plsa">pLSA</h5>
<p>即概率LSA，把LSA变成从概率的角度理解，一般采用的是EM的方式学习</p>
<p>由于pLSA的推导和求解一般涉及到<code>上帝算法</code> — <code>EM</code>（我把它认为是<code>魔鬼算法</code>，因为它原理简单，但是推导要命）在此不做深入讨论</p>
<h3 id="后记">后记</h3>
<p>上述例子均为博主本人生造的，因为网上关于LSA的代码示例很多只做到SVD分解部分，没具体到酉矩阵的应用。</p>
<p>上述实例的完整代码地址: <a href="https://github.com/SeanLee97/nlp_learning/blob/master/lsa/lsa.py" target="_blank" rel="noopener">github/nlp_learning/lsa</a></p>
<h3 id="refrence">Refrence</h3>
<p>[1] https://blog.csdn.net/KIDGIN7439/article/details/69831490</p>
<p>[2] 《数学之美》</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://seanlee97.github.io/2018/08/31/余弦定理和文本相似度/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sean lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/31/余弦定理和文本相似度/" itemprop="url">余弦定理和文本相似度</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-31T22:02:53+08:00">
                2018-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index">
                    <span itemprop="name">技术日志</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/31/余弦定理和文本相似度/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/31/余弦定理和文本相似度/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>数学中常用余弦定理来计算两条边的夹角，NLP中可以用余弦定理来计算文本相似度。</p>
<p>计算两个向量的余弦定理，求得的夹角 <span class="math inline">\(\theta\)</span> 越小，说明两个向量越接近，计算公式如下：</p>
<p><span class="math display">\[\begin{align*}
cos(\theta) &amp;= \frac{\underset{a}{\rightarrow} \cdot \underset{b}{\rightarrow}}{\left \| \underset{a}{\rightarrow} \right \| \left \| \underset{b}{\rightarrow} \right \|} \\
&amp;=  \frac{x_{1} \times x_{2} + y_{1} \times y_{2}}{\sqrt{x_{1}^{2} + y_{1}^{2}} \times \sqrt{x_{2}^{2} + y_{2}^{2}}} \\
&amp; where \underset{a}{\rightarrow} = (x_{1}, y_{1}),\ \underset{b}{\rightarrow} = (x_{2}, y_{2}) 
\end{align*}\]</span></p>
<p>同理如果将两篇文档分别表示成向量利用余弦定理计算不就能得到两篇文档的相关性了吗？ 因此我们第一步要做的就是对文档进行向量化表示，表示方法一般有两种：</p>
<ul>
<li>词袋模型（文本特征为词频）</li>
<li>tf-idf（文本特征为tfidf值)</li>
</ul>
<p>文档中每一个词对应一个维度，因此文档向量是多维的，因此需要对余弦定理泛化，使其能计算多维空间的距离，扩展公式为:</p>
<p><span class="math display">\[\begin{align*}
cos(\theta) &amp;= \frac{\underset{a}{\rightarrow} \cdot \underset{b}{\rightarrow}}{\left \| \underset{a}{\rightarrow} \right \| \left \| \underset{b}{\rightarrow} \right \|} \\
&amp;=  \frac{x_{1} \times y_{2} + x_{2} \times y_{2} + ... + x_{n} \times y_{n}}{\sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} \times \sqrt{y_{1}^{2} + y_{2}^{2} + ... + y_{n}^{2}}} \\
&amp; where \underset{a}{\rightarrow} = (x_{1}, x_{1}, ..., x_{n}),\ \underset{b}{\rightarrow} = (y_{2}, y_{2}, ..., y_{n}) 
\end{align*}\]</span></p>
<h4 id="代码实现">代码实现</h4>
<p>如果你熟悉<code>numpy</code>那么上面的计算就是小case了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sim</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel=<span class="string">'tfidf'</span>)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.kernel = kernel</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> jieba.lcut(sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词袋向量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_bow</span><span class="params">(self, docs)</span>:</span></span><br><span class="line">        bow = np.zeros([len(docs), len(self.word2idx)])</span><br><span class="line">        <span class="keyword">for</span> docidx, words <span class="keyword">in</span> enumerate(docs):</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">                    bow[docidx, self.word2idx[word]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> bow</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算tfidf    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_tfidf</span><span class="params">(self, docs)</span>:</span></span><br><span class="line">        tf = self.calc_bow(docs)</span><br><span class="line">        df = np.ones([<span class="number">1</span>, len(self.word2idx)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> docidx, words <span class="keyword">in</span> enumerate(docs):</span><br><span class="line">            tf[docidx] /= np.max(tf[docidx])</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">                    df[<span class="number">0</span>, self.word2idx[word]] += <span class="number">1</span></span><br><span class="line">        idf = np.log(len(docs)) - np.log(df)</span><br><span class="line">        tfidf = tf * idf</span><br><span class="line">        <span class="keyword">return</span> tfidf</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算余弦相似度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cos</span><span class="params">(self, vec1, vec2)</span>:</span></span><br><span class="line">        cos = np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2))</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            cos = np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2))</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            cos = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cos</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算文本相似度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(self, doc1, doc2)</span>:</span></span><br><span class="line">        words1 = self.tokenizer(doc1)</span><br><span class="line">        words2 = self.tokenizer(doc2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 求并集</span></span><br><span class="line">        words = set(words1) | set(words2)</span><br><span class="line">        <span class="comment"># word2idx = &#123;word1:0, word2:1, ...&#125;</span></span><br><span class="line">        self.word2idx = dict(zip(words, range(len(words))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.kernel == <span class="string">'tfidf'</span>:</span><br><span class="line">            feature = self.calc_tfidf</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature = self.calc_bow</span><br><span class="line"></span><br><span class="line">        vec = feature([words1, words2])</span><br><span class="line">        vec1 = vec[<span class="number">0</span>]</span><br><span class="line">        vec2 = vec[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.cos(vec1, vec2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    doc1 = <span class="string">"""计算机科学（英语：computer science，有时缩写为CS）是系统性研究信息与计算的理论基础以及它们在计算机系统中如何实现与应用的实用技术的学科。</span></span><br><span class="line"><span class="string">    [1] [2]它通常被形容为对那些创造、描述以及转换信息的算法处理的系统研究。</span></span><br><span class="line"><span class="string">    计算机科学包含很多分支领域；有些强调特定结果的计算，比如计算机图形学；</span></span><br><span class="line"><span class="string">    而有些是探讨计算问题的性质，比如计算复杂性理论；还有一些领域专注于怎样实现计算，比如编程语言理论是研究描述计算的方法，</span></span><br><span class="line"><span class="string">    而程序设计是应用特定的编程语言解决特定的计算问题，人机交互则是专注于怎样使计算机和计算变得有用、好用，以及随时随地为人所用。"""</span></span><br><span class="line"></span><br><span class="line">    doc2 = <span class="string">"""自然语言处理（英语：natural language processing，缩写作 NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。</span></span><br><span class="line"><span class="string">自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。"""</span></span><br><span class="line">    sim = Sim()</span><br><span class="line">    print(sim.similarity(doc1, doc2))</span><br></pre></td></tr></table></figure>
<h4 id="总结">总结</h4>
<p>NLP中涉及到计算应该下意识的想到将文本转换成空间向量表示，因为向量是能计算的而文本不能直接计算</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="sean lee" />
            
              <p class="site-author-name" itemprop="name">sean lee</p>
              <p class="site-description motion-element" itemprop="description">NLP / DL / Python / C++ | 爱我所爱</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index-1.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index-1.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/SeanLee97" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xmlee97@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/u/0/114423502510761945752" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/xmlee97" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sean lee</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>






    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">&nbsp;总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://seanlee97.disqus.com/count.js" async></script>
    

    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
